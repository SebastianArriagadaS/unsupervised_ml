{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "CYtBNclGpf_Q",
        "sPwnGJ5q_0lZ",
        "JMbPor8EKO4A"
      ],
      "authorship_tag": "ABX9TyNyLuAhNFvqgXzQMRpeXJJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebastianArriagadaS/unsupervised_ml/blob/main/Unsupervized_Image_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image classification using deep clustering\n",
        "The purpose of the project is to use unsupervised image classification techniques on the German traffic sign dataset. A comparison between different state-of-the-art models is required regarding the accuracy, A theoretical comparison justifying the choice of the model is also acceptable."
      ],
      "metadata": {
        "id": "VbF4mhC0Xj8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "0Fc1HQ8wl-aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRnlOhTzXrOv",
        "outputId": "0dbf7cf3-e085-4a92-b292-7b7c1b30176d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gbdhmEC2QHBj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,Dense,BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "j0Kwce8WXqHP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervized"
      ],
      "metadata": {
        "id": "CYtBNclGpf_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_class = 43"
      ],
      "metadata": {
        "id": "0koRsCZZS2as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "i6fdWkbvbI0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rescale=1/255.0, validation_split=0.2)"
      ],
      "metadata": {
        "id": "YtdITdTRbBtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/'\n",
        "df_train = pd.read_csv(path + 'Train.csv')\n",
        "df_train.head() "
      ],
      "metadata": {
        "id": "41-93IzUbHht",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "eb0d1158-2968-41b9-ffba-8ed10bfa559b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Width  Height  Roi.X1  Roi.Y1  Roi.X2  Roi.Y2  ClassId  \\\n",
              "0     27      26       5       5      22      20       20   \n",
              "1     28      27       5       6      23      22       20   \n",
              "2     29      26       6       5      24      21       20   \n",
              "3     28      27       5       6      23      22       20   \n",
              "4     28      26       5       5      23      21       20   \n",
              "\n",
              "                             Path  \n",
              "0  Train/20/00020_00000_00000.png  \n",
              "1  Train/20/00020_00000_00001.png  \n",
              "2  Train/20/00020_00000_00002.png  \n",
              "3  Train/20/00020_00000_00003.png  \n",
              "4  Train/20/00020_00000_00004.png  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87837135-6f96-4dc0-a5ee-49578ffac1da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>Height</th>\n",
              "      <th>Roi.X1</th>\n",
              "      <th>Roi.Y1</th>\n",
              "      <th>Roi.X2</th>\n",
              "      <th>Roi.Y2</th>\n",
              "      <th>ClassId</th>\n",
              "      <th>Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27</td>\n",
              "      <td>26</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>Train/20/00020_00000_00000.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28</td>\n",
              "      <td>27</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>Train/20/00020_00000_00001.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>26</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>24</td>\n",
              "      <td>21</td>\n",
              "      <td>20</td>\n",
              "      <td>Train/20/00020_00000_00002.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>28</td>\n",
              "      <td>27</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>22</td>\n",
              "      <td>20</td>\n",
              "      <td>Train/20/00020_00000_00003.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>26</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>21</td>\n",
              "      <td>20</td>\n",
              "      <td>Train/20/00020_00000_00004.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87837135-6f96-4dc0-a5ee-49578ffac1da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87837135-6f96-4dc0-a5ee-49578ffac1da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87837135-6f96-4dc0-a5ee-49578ffac1da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.shape)\n",
        "df_train = df_train[df_train['ClassId'] < nb_class]\n",
        "print(df_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niar9dmuqtOV",
        "outputId": "28415734-17ec-4b0e-ac8f-861c2de3a99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(39209, 8)\n",
            "(39209, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['Path']=df_train.Path.apply(lambda x: path + x)\n",
        "df_train['ClassId']=df_train.ClassId.astype(str)"
      ],
      "metadata": {
        "id": "tB-D_UPflOnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfL_56UJl1SH",
        "outputId": "47d7fc0f-2ece-4d87-aed8-2f44a4a00078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 39209 entries, 0 to 39208\n",
            "Data columns (total 8 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Width    39209 non-null  int64 \n",
            " 1   Height   39209 non-null  int64 \n",
            " 2   Roi.X1   39209 non-null  int64 \n",
            " 3   Roi.Y1   39209 non-null  int64 \n",
            " 4   Roi.X2   39209 non-null  int64 \n",
            " 5   Roi.Y2   39209 non-null  int64 \n",
            " 6   ClassId  39209 non-null  object\n",
            " 7   Path     39209 non-null  object\n",
            "dtypes: int64(6), object(2)\n",
            "memory usage: 2.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "width, height = 50,50\n",
        "trainDatagen = datagen.flow_from_dataframe(df_train, directory=None, x_col='Path', y_col='ClassId',\n",
        "                                           target_size=(width,height), class_mode = 'categorical', batch_size = 16, \n",
        "                                           subset='training')\n",
        "\n",
        "x, y = trainDatagen.next()\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuc0OgaFnfcL",
        "outputId": "700afda2-d8f0-4d67-9e52-434e09e8bfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 31368 validated image filenames belonging to 43 classes.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16, 50, 50, 3), (16, 43))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "jAlSNPoCo86u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/'\n",
        "df_test = pd.read_csv(path + 'Test.csv')\n",
        "df_test.head() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kiOg0PL2o9mu",
        "outputId": "3b18686a-2620-44ed-825c-4fa531348527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Width  Height  Roi.X1  Roi.Y1  Roi.X2  Roi.Y2  ClassId            Path\n",
              "0     53      54       6       5      48      49       16  Test/00000.png\n",
              "1     42      45       5       5      36      40        1  Test/00001.png\n",
              "2     48      52       6       6      43      47       38  Test/00002.png\n",
              "3     27      29       5       5      22      24       33  Test/00003.png\n",
              "4     60      57       5       5      55      52       11  Test/00004.png"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97eaaf63-aff0-4bcc-bbc0-e2b71cf38675\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>Height</th>\n",
              "      <th>Roi.X1</th>\n",
              "      <th>Roi.Y1</th>\n",
              "      <th>Roi.X2</th>\n",
              "      <th>Roi.Y2</th>\n",
              "      <th>ClassId</th>\n",
              "      <th>Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>53</td>\n",
              "      <td>54</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>48</td>\n",
              "      <td>49</td>\n",
              "      <td>16</td>\n",
              "      <td>Test/00000.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>42</td>\n",
              "      <td>45</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>36</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>Test/00001.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>48</td>\n",
              "      <td>52</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>43</td>\n",
              "      <td>47</td>\n",
              "      <td>38</td>\n",
              "      <td>Test/00002.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27</td>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>22</td>\n",
              "      <td>24</td>\n",
              "      <td>33</td>\n",
              "      <td>Test/00003.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>57</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>55</td>\n",
              "      <td>52</td>\n",
              "      <td>11</td>\n",
              "      <td>Test/00004.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97eaaf63-aff0-4bcc-bbc0-e2b71cf38675')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97eaaf63-aff0-4bcc-bbc0-e2b71cf38675 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97eaaf63-aff0-4bcc-bbc0-e2b71cf38675');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test.shape)\n",
        "df_test = df_test[df_test['ClassId'] < nb_class]\n",
        "print(df_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP_IggEgrGU2",
        "outputId": "4df22ecb-e553-440f-8c73-f98ba4466ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12630, 8)\n",
            "(12630, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['Path']=df_test.Path.apply(lambda x: path + x)\n",
        "df_test['ClassId']=df_test.ClassId.astype(str)"
      ],
      "metadata": {
        "id": "bRBNjwLmpDAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiCVqO4cpEbD",
        "outputId": "c5e0ba20-ac85-4382-c234-a68de4804542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 39209 entries, 0 to 39208\n",
            "Data columns (total 8 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   Width    39209 non-null  int64 \n",
            " 1   Height   39209 non-null  int64 \n",
            " 2   Roi.X1   39209 non-null  int64 \n",
            " 3   Roi.Y1   39209 non-null  int64 \n",
            " 4   Roi.X2   39209 non-null  int64 \n",
            " 5   Roi.Y2   39209 non-null  int64 \n",
            " 6   ClassId  39209 non-null  object\n",
            " 7   Path     39209 non-null  object\n",
            "dtypes: int64(6), object(2)\n",
            "memory usage: 2.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "width, height = 50,50\n",
        "testDatagen = datagen.flow_from_dataframe(df_test, directory=None, x_col='Path', y_col='ClassId',\n",
        "                                           target_size=(width,height), class_mode = 'categorical', batch_size = 16, \n",
        "                                           subset='training')\n",
        "\n",
        "x, y = testDatagen.next()\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88h4QuuBpKwa",
        "outputId": "8df70e94-efab-4718-f151-6f83a6d65692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10104 validated image filenames belonging to 43 classes.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16, 50, 50, 3), (16, 43))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arbiratory CNN"
      ],
      "metadata": {
        "id": "q_XMRqfcpimN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers"
      ],
      "metadata": {
        "id": "kM6Us7_wsf_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential() #Sequential Model\n",
        "\n",
        "#ConvLayer(64 filters) + MaxPooling + BatchNormalization + Dropout\n",
        "model.add(layers.Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',input_shape=(50, 50, 3)))\n",
        "model.add(layers.MaxPool2D(strides=2))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "#Flatten\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "#Dense layer with 1000 hidden units\n",
        "model.add(layers.Dense(1000,activation='relu'))\n",
        "\n",
        "#Softmax layer for output\n",
        "model.add(layers.Dense(nb_class,activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWhDGC4Qphv1",
        "outputId": "47db17eb-4582-4e82-aa5a-9d93661c4f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 50, 50, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 25, 25, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 25, 25, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 25, 25, 32)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 20000)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              20001000  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 43)                43043     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,045,067\n",
            "Trainable params: 20,045,003\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "history= model.fit(trainDatagen, epochs=20, batch_size=64,\n",
        "                 validation_data=testDatagen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "O62m7ISPpr3a",
        "outputId": "2bdaa3fd-8944-4756-b4e3-641dc6266203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " 180/1961 [=>............................] - ETA: 3:42:31 - loss: 3.2381 - accuracy: 0.4868"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3f98ea1f9146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m history= model.fit(trainDatagen, epochs=20, batch_size=64,\n\u001b[0;32m----> 6\u001b[0;31m                  validation_data=testDatagen)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Modelaccuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Train\",\"Test\"],loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend([\"Train\",\"Test\"],loc=\"upper left\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4FpnW_bVqJqR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "fa1acaba-4e01-4efc-bde4-b97582cc2af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b92305408b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Modelaccuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervized"
      ],
      "metadata": {
        "id": "eYd1bAdmapXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import time\n",
        "import os, os.path\n",
        "from os import listdir\n",
        "import random\n",
        "import cv2\n",
        "import glob\n",
        "import keras\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "tBcGv4EjrrWB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load test/train Data"
      ],
      "metadata": {
        "id": "tCTjWS-Jl67Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_stats(directory_path):    \n",
        "    # dictionary where we will store the stats\n",
        "    stats = []\n",
        "    \n",
        "    for sub_directory in tqdm(directory_path):\n",
        "      file_names = [file for file in listdir(sub_directory)]\n",
        "      file_count = len(file_names)\n",
        "      stats.append({\"Image count\": file_count, \n",
        "                      \"Folder name\": os.path.basename(sub_directory),\n",
        "                      \"File names\": file_names})\n",
        "\n",
        "    \n",
        "    df = pd.DataFrame(stats)\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "v-0tSNyCrJ49"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_labels(DIR, n_to_load = 40000):\n",
        "  directory_list = listdir(DIR)\n",
        "\n",
        "  directory_path = []\n",
        "  for l in directory_list:\n",
        "    directory_path.append(DIR + l)\n",
        "  dataset = dataset_stats(directory_path)\n",
        "  dataset[\"Folder name\"] = pd.to_numeric(dataset[\"Folder name\"])\n",
        "  dataset = dataset.sort_values(by = \"Folder name\")\n",
        "  dataset[\"Folder name\"] = dataset[\"Folder name\"].astype(str)\n",
        "\n",
        "  path = DIR[:105]\n",
        "  df_train = pd.read_csv(path + DIR[105:-1] + '.csv')\n",
        "  df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  list_images = []\n",
        "  labels = []\n",
        "  for image in tqdm(df_train.loc[20000:, 'Path']):\n",
        "    img = plt.imread(path + image)\n",
        "    img = cv2.resize(img, (32,32))\n",
        "    list_images.append(img)\n",
        "\n",
        "    labels.append(image[6:-22])\n",
        "\n",
        "  images = np.array(list_images)\n",
        "  labels = np.array(labels)\n",
        "  print(images.shape, labels.shape)\n",
        "\n",
        "  return images, labels"
      ],
      "metadata": {
        "id": "YjP0pjUG4MLk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_random_images(images, labels, number_of_images_to_show=2):\n",
        "\n",
        "    for code in list(set(labels)):\n",
        "\n",
        "        indicies = [i for i, label in enumerate(labels) if label == code]\n",
        "        random_indicies = [random.choice(indicies) for i in range(number_of_images_to_show)]\n",
        "        figure, axis = plt.subplots(1, number_of_images_to_show)\n",
        "\n",
        "        print(\"{} random images for code {}\".format(number_of_images_to_show, code))\n",
        "\n",
        "        for image in range(number_of_images_to_show):\n",
        "            axis[image].imshow(images[random_indicies[image]])\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Ig4Im7lR7H3E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = \"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/Train/\"\n",
        "directory_list = listdir(DIR)\n",
        "\n",
        "directory_path = []\n",
        "for l in directory_list:\n",
        "  directory_path.append(DIR + l)\n",
        "dataset = dataset_stats(directory_path)\n",
        "dataset[\"Folder name\"] = pd.to_numeric(dataset[\"Folder name\"])\n",
        "dataset = dataset.sort_values(by = \"Folder name\")\n",
        "dataset[\"Folder name\"] = dataset[\"Folder name\"].astype(str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "oFyZ2Ft1b74W",
        "outputId": "9e99deda-0163-4606-fbe2-0ed9a7983b0a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 6/43 [01:00<06:14, 10.11s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-341d119ebcc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectory_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mdirectory_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Folder name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Folder name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Folder name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2d670e3d413e>\u001b[0m in \u001b[0;36mdataset_stats\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msub_directory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mfile_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       stats.append({\"Image count\": file_count, \n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "__e2u2gH7PHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = \"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/Train/\"\n",
        "images, labels = load_images_labels(DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sfAedP05EEr",
        "outputId": "6f2d02c1-5732-4c01-aa48-35e8bef4e133"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 43/43 [01:14<00:00,  1.72s/it]\n",
            "100%|██████████| 19209/19209 [1:21:54<00:00,  3.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19209, 32, 32, 3) (19209,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/images2.npy', images)\n",
        "np.save('/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/labels2.npy', labels)"
      ],
      "metadata": {
        "id": "30b73PjdkcLJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_images(images, labels)"
      ],
      "metadata": {
        "id": "k872r7h52kEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test"
      ],
      "metadata": {
        "id": "UXh_mvyg8im9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = \"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/Test/\"\n",
        "\n",
        "images_test = []\n",
        "for image in tqdm([f for f in listdir(DIR) if os.path.isfile(os.path.join(DIR, f))]):\n",
        "    img = plt.imread(DIR + image)\n",
        "    img = cv2.resize(img, (32,32))\n",
        "    images_test.append(img)\n",
        "\n",
        "images_test = np.array(images_test)\n",
        "print(images_test.shape)"
      ],
      "metadata": {
        "id": "rFXoOB2u8kVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3cd940-9e43-47e8-961b-be899d01f046"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12630/12630 [00:26<00:00, 479.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12630, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/test_images.npy', images_test)"
      ],
      "metadata": {
        "id": "RqGQJd46qzZB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concat arrays and save"
      ],
      "metadata": {
        "id": "JFL6K_MVQVMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train1 = np.load(\"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/images.npy\")\n",
        "train2 = np.load(\"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/images2.npy\")\n",
        "\n",
        "label1 = np.load(\"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/labels.npy\")\n",
        "label2 = np.load(\"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/labels2.npy\")"
      ],
      "metadata": {
        "id": "vDcJnRvvecNH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = np.concatenate((train1, train2), axis=0)\n",
        "print(train.shape)\n",
        "\n",
        "labels = np.concatenate((label1, label2), axis=0)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFx3jY7-PmDR",
        "outputId": "8517ae99-5cae-4a45-9805-2557ac8ebe5a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(39210, 32, 32, 3)\n",
            "(39210,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/train_images.npy\", train)\n",
        "np.save(\"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/train_labels.npy\", labels)"
      ],
      "metadata": {
        "id": "o9lFsUSQQFbW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load"
      ],
      "metadata": {
        "id": "o_Enik2Cyy4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = np.load(\"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/train_images.npy\")\n",
        "labels = np.load(\"/content/gdrive/MyDrive/Cours/MAM5A/ELTE/Advanced Machine Learning/Unsupervized Clustering on Image/data/train_labels.npy\")"
      ],
      "metadata": {
        "id": "74gFms_oy04z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing "
      ],
      "metadata": {
        "id": "pXAMVg0T9rZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now convert the images and labels to NumPy arrays to make processing them easier. We then normaise the images before passing them on to VGG19"
      ],
      "metadata": {
        "id": "W1mI6Gs5-dCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise_images(images, labels):\n",
        "    # Convert to numpy arrays\n",
        "    images = np.array(images, dtype=np.float32)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Normalise the images\n",
        "    images /= 255\n",
        "    \n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "vlYdO1dW9veb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = normalise_images(images, labels)"
      ],
      "metadata": {
        "id": "U0YjsL359x6F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all the photos and their labels in arrays. It's time to shuffle them around, and split them to two different sets : training, validation and we already have testing.\n",
        "\n",
        "We'll be using the train_test_split function from sklearn which will also shuffle the data around for us, since it's currently in order."
      ],
      "metadata": {
        "id": "mKJq6ziA-hAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=728)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "8cQFkiVL-tRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f4a9037-558f-4413-c562-4221df19dac6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(31368, 32, 32, 3) (7842, 32, 32, 3) (31368,) (7842,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First try : CNN encoding + PCA + Kmeans"
      ],
      "metadata": {
        "id": "sPwnGJ5q_0lZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pretrained Models to encode"
      ],
      "metadata": {
        "id": "IldtSEOFaruw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16, VG19, ResNet50\n",
        "\n",
        "We'll now load up the keras models with the imagenet weights. We'll remove the top dense layers, since we won't need to classify things here, and we just want these encoded features from the images."
      ],
      "metadata": {
        "id": "AeH_Tgkn_cc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the models with ImageNet weights\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n",
        "vgg16_model = VGG16(include_top=False, weights=\"imagenet\", input_shape=(32,32,3))\n",
        "vgg19_model = VGG19(include_top=False, weights=\"imagenet\", input_shape=(32,32,3))\n",
        "resnet50_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(32,32,3))"
      ],
      "metadata": {
        "id": "NSn0RA3Xaqsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9caa3b-e97b-4692-ea6f-23f74c95eb16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 3s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 5s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 5s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The covnet models will give us 3D vectors that represent the image. We need to flatten these for the clustering algorithms to start working with them."
      ],
      "metadata": {
        "id": "T-rCQ7p3AyEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def covnet_transform(covnet_model, raw_images):\n",
        "    # Pass our training data through the network\n",
        "    pred = covnet_model.predict(raw_images)\n",
        "\n",
        "    # Flatten the array\n",
        "    flat = pred.reshape(raw_images.shape[0], -1)\n",
        "    \n",
        "    return flat"
      ],
      "metadata": {
        "id": "T5nNX98RAuPB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_output = covnet_transform(vgg16_model, X_train)\n",
        "print(\"VGG16 flattened output has {} features\".format(vgg16_output.shape[1]))\n",
        "\n",
        "vgg19_output = covnet_transform(vgg19_model, X_train)\n",
        "print(\"VGG19 flattened output has {} features\".format(vgg19_output.shape[1]))\n",
        "\n",
        "resnet50_output = covnet_transform(resnet50_model, X_train)\n",
        "print(\"ResNet50 flattened output has {} features\".format(resnet50_output.shape[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igR8MuT1_gqv",
        "outputId": "58beb2ed-6b46-4ac7-8e4d-49cdefa0c550"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "981/981 [==============================] - 15s 7ms/step\n",
            "VGG16 flattened output has 512 features\n",
            "981/981 [==============================] - 9s 9ms/step\n",
            "VGG19 flattened output has 512 features\n",
            "981/981 [==============================] - 10s 9ms/step\n",
            "ResNet50 flattened output has 2048 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above cell shows us the number of features each covnet gives to a single image. When we compare these to the original size of the image 32 x 32 x 3 = 3072 pixels/features, we can see that this is a large reduction in what the clustering algorithms will have to work with.\n",
        "\n",
        "Hopefully these reduces number of feature are represent more meaningful features in the image structure."
      ],
      "metadata": {
        "id": "FlmpFxeLBMRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PCA"
      ],
      "metadata": {
        "id": "zdk9G_GgBcqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While k-means clustering has coped with these numbers, Gaussian Mixture Modelling has not and the computer consistently ran out of memory and struggled to produce results.\n",
        "\n",
        "We therefore look to PCA for dimensionality reduction, so that our clustering algorithms can cope."
      ],
      "metadata": {
        "id": "MonxxXukBfdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that creates a PCA instance, fits it to the data and returns the instance\n",
        "def create_fit_PCA(data, n_components=None):\n",
        "    p = PCA(n_components=n_components, random_state=728)\n",
        "    p.fit(data)\n",
        "    return p"
      ],
      "metadata": {
        "id": "gX_NSeGtBNOi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PCA instances for each covnet output\n",
        "vgg16_pca = create_fit_PCA(vgg16_output)\n",
        "vgg19_pca = create_fit_PCA(vgg19_output)\n",
        "resnet50_pca = create_fit_PCA(resnet50_output)"
      ],
      "metadata": {
        "id": "-wHRA0AABqaS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the cumulative explained variance of PCA components\n",
        "# This will help us decide how many components we should reduce our features to\n",
        "def pca_cumsum_plot(pca):\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('number of components')\n",
        "    plt.ylabel('cumulative explained variance')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "V5VLM5qdBrxn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the cumulative explained variance for each covnet\n",
        "pca_cumsum_plot(vgg16_pca)\n",
        "pca_cumsum_plot(vgg19_pca)\n",
        "pca_cumsum_plot(resnet50_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "EhER5mZKB2ev",
        "outputId": "f2892a03-5cf9-4163-cd9a-87fe3c8fd381"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the gaphs above, we can see that PCA can explain almost all the variance in as many dimensions as there are samples.\n",
        "\n",
        "It is also interesting to note the difference in shape between the VGG graphs and the ResNet one. This is probably due to the fact that ResNet only had 2048 dimensions to start with, while VGGs had 25,088"
      ],
      "metadata": {
        "id": "guU9WH8ZCHua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA transformations of covnet outputs\n",
        "vgg16_output_pca = vgg16_pca.transform(vgg16_output)\n",
        "vgg19_output_pca = vgg19_pca.transform(vgg19_output)\n",
        "resnet50_output_pca = resnet50_pca.transform(resnet50_output)"
      ],
      "metadata": {
        "id": "9Cjlv54iCGvL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kmeans"
      ],
      "metadata": {
        "id": "MsFfGTybCjfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a couple of functions that would create and fit KMeans."
      ],
      "metadata": {
        "id": "w8MSV1m9Cpk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_kmeans(data, number_of_clusters=43):\n",
        "    # n_jobs is set to -1 to use all available CPU cores. This makes a big difference on an 8-core CPU\n",
        "    # especially when the data size gets much bigger. #perfMatters\n",
        "    \n",
        "    k = KMeans(n_clusters=number_of_clusters, random_state=728)\n",
        "\n",
        "    # Let's do some timings to see how long it takes to train.\n",
        "    start = time.time()\n",
        "\n",
        "    # Train it up\n",
        "    k.fit(data)\n",
        "\n",
        "    # Stop the timing \n",
        "    end = time.time()\n",
        "\n",
        "    # And see how long that took\n",
        "    print(\"Training took {} seconds\".format(end-start))\n",
        "    \n",
        "    return k"
      ],
      "metadata": {
        "id": "XOrXAEPJCkdH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's pass the data into the algorithm and predict who lies in which cluster. \n",
        "# Since we're using the same data that we trained it on, this should give us the training results.\n",
        "\n",
        "# Here we create and fit a KMeans model with the PCA outputs\n",
        "print(\"KMeans (PCA): \\n\")\n",
        "\n",
        "print(\"VGG16\")\n",
        "K_vgg16_pca = create_train_kmeans(vgg16_output_pca)\n",
        "\n",
        "print(\"\\nVGG19\")\n",
        "K_vgg19_pca = create_train_kmeans(vgg19_output_pca)\n",
        "\n",
        "print(\"\\nResNet50\")\n",
        "K_resnet50_pca = create_train_kmeans(resnet50_output_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXwe88AzDUBM",
        "outputId": "25583a09-1405-40c1-deed-7b89d25e00a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans (PCA): \n",
            "\n",
            "VGG16\n",
            "Training took 134.93922328948975 seconds\n",
            "\n",
            "VGG19\n",
            "Training took 104.1550703048706 seconds\n",
            "\n",
            "ResNet50\n",
            "Training took 994.7524244785309 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also create models for the covnet outputs without PCA for comparison\n",
        "print(\"KMeans: \\n\")\n",
        "\n",
        "print(\"VGG16:\")\n",
        "K_vgg16 = create_train_kmeans(vgg16_output)\n",
        "\n",
        "print(\"\\nVGG19:\")\n",
        "K_vgg19 = create_train_kmeans(vgg19_output)\n",
        "\n",
        "print(\"\\nResNet50:\")\n",
        "K_resnet50 = create_train_kmeans(resnet50_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_-Oc1wBFjC9",
        "outputId": "9b2cdf8f-ec13-4dcd-eb33-4e39e3411ee7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans: \n",
            "\n",
            "VGG16:\n",
            "Training took 79.19084739685059 seconds\n",
            "\n",
            "VGG19:\n",
            "Training took 85.91106629371643 seconds\n",
            "\n",
            "ResNet50:\n",
            "Training took 262.2659242153168 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we get the custer model predictions\n",
        "\n",
        "# KMeans with PCA outputs\n",
        "k_vgg16_pred_pca = K_vgg16_pca.predict(vgg16_output_pca)\n",
        "k_vgg19_pred_pca = K_vgg19_pca.predict(vgg19_output_pca)\n",
        "k_resnet50_pred_pca = K_resnet50_pca.predict(resnet50_output_pca)\n",
        "\n",
        "# KMeans with CovNet outputs (without PCA)\n",
        "k_vgg16_pred = K_vgg16.predict(vgg16_output)\n",
        "k_vgg19_pred = K_vgg19.predict(vgg19_output)\n",
        "k_resnet50_pred = K_resnet50.predict(resnet50_output)"
      ],
      "metadata": {
        "id": "ri-EHncjE27E"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clustering algorith does not detect which images are which in term how labels, it only groups images that look alike together and assigns them a number arbitrarily.\n",
        "\n",
        "We now need to count how many of each label are in each cluster, this way we can take a look and if sufficient eperation has happened we can quicly see which cluster is which label. So let's write a function that does that.\n"
      ],
      "metadata": {
        "id": "ISZMexk3E92H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trying to get the cluster names back"
      ],
      "metadata": {
        "id": "JMbPor8EKO4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_label_count(clusters, labels):\n",
        "    \n",
        "    count = {}\n",
        "    \n",
        "    # Get unique clusters and labels\n",
        "    unique_clusters = list(set(clusters))\n",
        "    unique_labels = list(set(labels))\n",
        "    \n",
        "    # Create counter for each cluster/label combination and set it to 0\n",
        "    for cluster in unique_clusters:\n",
        "        count[cluster] = {}\n",
        "        \n",
        "        for label in unique_labels:\n",
        "            count[cluster][label] = 0\n",
        "    \n",
        "    # Let's count\n",
        "    for i in range(len(clusters)):\n",
        "        count[clusters[i]][labels[i]] +=1\n",
        "    \n",
        "    cluster_df = pd.DataFrame(count)\n",
        "    \n",
        "    return cluster_df"
      ],
      "metadata": {
        "id": "VfVBpnZEE9dd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster counting for VGG16 KMeans\n",
        "vgg16_cluster_count = cluster_label_count(k_vgg16_pred, y_train)\n",
        "vgg16_cluster_count_pca = cluster_label_count(k_vgg16_pred_pca, y_train)\n",
        "\n",
        "# VGG19 KMeans\n",
        "vgg19_cluster_count = cluster_label_count(k_vgg19_pred, y_train)\n",
        "vgg19_cluster_count_pca = cluster_label_count(k_vgg19_pred_pca, y_train)\n",
        "\n",
        "# ResNet50 KMeans\n",
        "resnet_cluster_count = cluster_label_count(k_resnet50_pred, y_train)\n",
        "resnet_cluster_count_pca = cluster_label_count(k_resnet50_pred_pca, y_train)"
      ],
      "metadata": {
        "id": "yGma4l35GFMi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accuracy"
      ],
      "metadata": {
        "id": "PiH7sW_teu5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"KMeans VGG16: \")\n",
        "cm_argmax = vgg16_cluster_count.idxmax(axis=0)\n",
        "pred_ = np.array([cm_argmax[i] for i in k_vgg16_pred])\n",
        "print(accuracy_score(y_train,pred_))\n",
        "\n",
        "print(\"KMeans VGG16 (PCA): \")\n",
        "cm_argmax = vgg16_cluster_count_pca.idxmax(axis=0)\n",
        "pred_ = np.array([cm_argmax[i] for i in k_vgg16_pred_pca])\n",
        "print(accuracy_score(y_train,pred_))\n",
        "\n",
        "############\n",
        "print(\"\\n KMeans VGG19: \")\n",
        "cm_argmax = vgg19_cluster_count.idxmax(axis=0)\n",
        "pred_ = np.array([cm_argmax[i] for i in k_vgg19_pred])\n",
        "print(accuracy_score(y_train,pred_))\n",
        "\n",
        "print(\"KMeans VGG16: \")\n",
        "cm_argmax = vgg19_cluster_count_pca.idxmax(axis=0)\n",
        "pred_ = np.array([cm_argmax[i] for i in k_vgg19_pred_pca])\n",
        "print(accuracy_score(y_train,pred_))\n",
        "\n",
        "###########\n",
        "\n",
        "print(\"\\n KMeans Resnet50: \")\n",
        "cm_argmax = resnet_cluster_count.idxmax(axis=0)\n",
        "pred_ = np.array([cm_argmax[i] for i in k_resnet50_pred])\n",
        "print(accuracy_score(y_train,pred_))\n",
        "\n",
        "print(\"Kmeans Resnet50 (PCA): \")\n",
        "cm_argmax = resnet_cluster_count_pca.idxmax(axis=0)\n",
        "pred_ = np.array([cm_argmax[i] for i in k_resnet50_pred_pca])\n",
        "print(accuracy_score(y_train,pred_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp8IsrXlGVL3",
        "outputId": "aa07c495-cd14-426c-9ac1-cbfdc350649e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans VGG16: \n",
            "0.1499617444529457\n",
            "KMeans VGG16 (PCA): \n",
            "0.15155572558020913\n",
            "\n",
            " KMeans VGG19: \n",
            "0.14042973731191022\n",
            "KMeans VGG16: \n",
            "0.14753889313950522\n",
            "\n",
            " KMeans Resnet50: \n",
            "0.07625605712828361\n",
            "Kmeans Resnet50 (PCA): \n",
            "0.16153404743687835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty bad even on training Data. I'll search for different methods."
      ],
      "metadata": {
        "id": "k9wfL3iA__MK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second Try : IIC on Mnist Dataset"
      ],
      "metadata": {
        "id": "4tTYILlWAJjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjKTiBdmlzuS",
        "outputId": "85167ce8-ad0c-4e41-f323-e9798d17d866"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.8/dist-packages (0.18.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data"
      ],
      "metadata": {
        "id": "Ndu3Dlpre7Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "def mnist_x(x_orig, mdl_input_dims, is_training):\n",
        "\n",
        "    # rescale to [0, 1]\n",
        "    x_orig = tf.cast(x_orig, dtype=tf.float32) / x_orig.dtype.max\n",
        "\n",
        "    # get common shapes\n",
        "    height_width = mdl_input_dims[:-1]\n",
        "    n_chans = mdl_input_dims[-1]\n",
        "\n",
        "    # training transformations\n",
        "    if is_training:\n",
        "        x1 = tf.image.central_crop(x_orig, np.mean(20 / np.array(x_orig.shape.as_list()[1:-1])))\n",
        "        x2 = tf.image.random_crop(x_orig, tf.concat((tf.shape(x_orig)[:1], [20, 20], [n_chans]), axis=0))\n",
        "        x = tf.stack([x1, x2])\n",
        "        x = tf.transpose(x, [1, 0, 2, 3, 4])\n",
        "        i = tf.squeeze(tf.random.categorical([[1., 1.]], tf.shape(x)[0]))\n",
        "        x = tf.map_fn(lambda y: y[0][y[1]], (x, i), dtype=tf.float32)\n",
        "        x = tf.image.resize(x, height_width)\n",
        "\n",
        "    # testing transformations\n",
        "    else:\n",
        "        x = tf.image.central_crop(x_orig, np.mean(20 / np.array(x_orig.shape.as_list()[1:-1])))\n",
        "        x = tf.image.resize(x, height_width)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def mnist_gx(x_orig, mdl_input_dims, is_training, sample_repeats):\n",
        "\n",
        "    # if not training, return a constant value--it will unused but needs to be same shape to avoid TensorFlow errors\n",
        "    if not is_training:\n",
        "        return tf.zeros([0] + mdl_input_dims)\n",
        "\n",
        "    # rescale to [0, 1]\n",
        "    x_orig = tf.cast(x_orig, dtype=tf.float32) / x_orig.dtype.max\n",
        "\n",
        "    # repeat samples accordingly\n",
        "    x_orig = tf.tile(x_orig, [sample_repeats] + [1] * len(x_orig.shape.as_list()[1:]))\n",
        "\n",
        "    # get common shapes\n",
        "    height_width = mdl_input_dims[:-1]\n",
        "    n_chans = mdl_input_dims[-1]\n",
        "\n",
        "    # random rotation\n",
        "    rad = 2 * np.pi * 25 / 360\n",
        "    x_rot = tfa.image.rotate(x_orig, tf.random.uniform(shape=tf.shape(x_orig)[:1], minval=-rad, maxval=rad))\n",
        "    gx = tf.stack([x_orig, x_rot])\n",
        "    gx = tf.transpose(gx, [1, 0, 2, 3, 4])\n",
        "    i = tf.squeeze(tf.random.categorical([[1., 1.]], tf.shape(gx)[0]))\n",
        "    gx = tf.map_fn(lambda y: y[0][y[1]], (gx, i), dtype=tf.float32)\n",
        "\n",
        "    # random crops\n",
        "    x1 = tf.image.random_crop(gx, tf.concat((tf.shape(x_orig)[:1], [16, 16], [n_chans]), axis=0))\n",
        "    x2 = tf.image.random_crop(gx, tf.concat((tf.shape(x_orig)[:1], [20, 20], [n_chans]), axis=0))\n",
        "    x3 = tf.image.random_crop(gx, tf.concat((tf.shape(x_orig)[:1], [24, 24], [n_chans]), axis=0))\n",
        "    gx = tf.stack([tf.image.resize(x1, height_width),\n",
        "                   tf.image.resize(x2, height_width),\n",
        "                   tf.image.resize(x3, height_width)])\n",
        "    gx = tf.transpose(gx, [1, 0, 2, 3, 4])\n",
        "    i = tf.squeeze(tf.random.categorical([[1., 1., 1.]], tf.shape(gx)[0]))\n",
        "    gx = tf.map_fn(lambda y: y[0][y[1]], (gx, i), dtype=tf.float32)\n",
        "\n",
        "    # apply random adjustments\n",
        "    def rand_adjust(img):\n",
        "        img = tf.image.random_brightness(img, 0.4)\n",
        "        img = tf.image.random_contrast(img, 0.6, 1.4)\n",
        "        if img.shape.as_list()[-1] == 3:\n",
        "            img = tf.image.random_saturation(img, 0.6, 1.4)\n",
        "            img = tf.image.random_hue(img, 0.125)\n",
        "        return img\n",
        "\n",
        "    gx = tf.map_fn(lambda y: rand_adjust(y), gx, dtype=tf.float32)\n",
        "\n",
        "    return gx\n",
        "\n",
        "\n",
        "def pre_process_data(ds, info, is_training, **kwargs):\n",
        "    \"\"\"\n",
        "    :param ds: TensorFlow Dataset object\n",
        "    :param info: TensorFlow DatasetInfo object\n",
        "    :param is_training: indicator to pre-processing function\n",
        "    :return: the passed in data set with map pre-processing applied\n",
        "    \"\"\"\n",
        "    # apply pre-processing function for given data set and run-time conditions\n",
        "    if info.name == 'mnist':\n",
        "        return ds.map(lambda d: {'x': mnist_x(d['image'],\n",
        "                                              mdl_input_dims=kwargs['mdl_input_dims'],\n",
        "                                              is_training=is_training),\n",
        "                                 'gx': mnist_gx(d['image'],\n",
        "                                                mdl_input_dims=kwargs['mdl_input_dims'],\n",
        "                                                is_training=is_training,\n",
        "                                                sample_repeats=kwargs['num_repeats']),\n",
        "                                 'label': d['label']},\n",
        "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    else:\n",
        "        raise Exception('Unsupported data set: ' + info.name)\n",
        "\n",
        "\n",
        "def configure_data_set(ds, info, batch_size, is_training, **kwargs):\n",
        "    \"\"\"\n",
        "    :param ds: TensorFlow data set object\n",
        "    :param info: TensorFlow DatasetInfo object\n",
        "    :param batch_size: batch size\n",
        "    :param is_training: indicator to pre-processing function\n",
        "    :return: a configured TensorFlow data set object\n",
        "    \"\"\"\n",
        "    # enable shuffling and repeats\n",
        "    ds = ds.shuffle(10 * batch_size, reshuffle_each_iteration=True).repeat(1)\n",
        "\n",
        "    # batch the data before pre-processing\n",
        "    ds = ds.batch(batch_size)\n",
        "\n",
        "    # pre-process the data set\n",
        "    with tf.device('/cpu:0'):\n",
        "        ds = pre_process_data(ds, info, is_training, **kwargs)\n",
        "\n",
        "    # enable prefetch\n",
        "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "def load(data_set_name, **kwargs):\n",
        "    \"\"\"\n",
        "    :param data_set_name: data set name--call tfds.list_builders() for options\n",
        "    :return:\n",
        "        train_ds: TensorFlow Dataset object for the training data\n",
        "        test_ds: TensorFlow Dataset object for the testing data\n",
        "        info: data set info object\n",
        "    \"\"\"\n",
        "    # get data and its info\n",
        "    ds, info = tfds.load(name=data_set_name, split=tfds.Split.ALL, with_info=True)\n",
        "\n",
        "    # configure the data sets\n",
        "    if 'train' in info.splits:\n",
        "        train_ds = configure_data_set(ds=ds, info=info, is_training=True, **kwargs)\n",
        "    else:\n",
        "        train_ds = None\n",
        "    if 'test' in info.splits:\n",
        "        test_ds = configure_data_set(ds=ds, info=info, is_training=False, **kwargs)\n",
        "    else:\n",
        "        test_ds = None\n",
        "\n",
        "    return train_ds, test_ds, info"
      ],
      "metadata": {
        "id": "QLdHsha1fDg-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Graph"
      ],
      "metadata": {
        "id": "InnXfurIgxz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# set trainable variable initialization routines\n",
        "KERNEL_INIT = tf.keras.initializers.he_uniform()\n",
        "WEIGHT_INIT = tf.random_normal_initializer(mean=0.0, stddev=0.01)\n",
        "BIAS_INIT = tf.constant_initializer(0.0)\n",
        "\n",
        "\n",
        "def convolution_layer(x, kernel_size, num_out_channels, activation, batch_norm, is_training, name):\n",
        "    \"\"\"\n",
        "    :param x: input data\n",
        "    :param kernel_size: convolution kernel size\n",
        "    :param num_out_channels: number of output channels\n",
        "    :param activation: non-linearity\n",
        "    :param batch_norm: whether to use batch norm\n",
        "    :param is_training: whether we are training or testing (used by batch normalization)\n",
        "    :param name: variable scope name (empowers variable reuse)\n",
        "    :return: layer output\n",
        "    \"\"\"\n",
        "    # run convolution layer\n",
        "    x = tf.compat.v1.layers.conv2d(inputs=x,\n",
        "                         filters=num_out_channels,\n",
        "                         kernel_size=[kernel_size] * 2,\n",
        "                         strides=[1, 1],\n",
        "                         padding='same',\n",
        "                         activation=None,\n",
        "                         use_bias=True,\n",
        "                         kernel_initializer=KERNEL_INIT,\n",
        "                         bias_initializer=BIAS_INIT,\n",
        "                         name=name)\n",
        "\n",
        "    # run batch norm if specified\n",
        "    if batch_norm:\n",
        "        x = tf.compat.v1.layers.BatchNormalization()(x)\n",
        "\n",
        "    # run activation\n",
        "    x = activation(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def max_pooling_layer(x, pool_size, strides, name):\n",
        "    \"\"\"\n",
        "    :param x: input data\n",
        "    :param pool_size: pooling kernel size\n",
        "    :param strides: pooling stride length\n",
        "    :param name: variable scope name (empowers variable reuse)\n",
        "    :return: layer output\n",
        "    \"\"\"\n",
        "    # run max pooling\n",
        "    x = tf.compat.v1.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=strides, padding='same', name=name)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def fully_connected_layer(x, num_outputs, activation, is_training, name):\n",
        "    \"\"\"\n",
        "    :param x: input data\n",
        "    :param num_outputs: number of outputs\n",
        "    :param activation: non-linearity\n",
        "    :param is_training: whether we are training or testing (used by batch normalization)\n",
        "    :param name: variable scope name (empowers variable reuse)\n",
        "    :return: layer output\n",
        "    \"\"\"\n",
        "    # run dense layer\n",
        "    x = tf.compat.v1.layers.dense(inputs=x,\n",
        "                        units=num_outputs,\n",
        "                        activation=None,\n",
        "                        use_bias=True,\n",
        "                        kernel_initializer=WEIGHT_INIT,\n",
        "                        bias_initializer=BIAS_INIT,\n",
        "                        name=name)\n",
        "\n",
        "    # run batch norm\n",
        "    x = tf.contrib.layers.batch_norm(inputs=x, activation_fn=activation, is_training=is_training)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class IICGraph(object):\n",
        "    def __init__(self, config='B', batch_norm=True, fan_out_init=64):\n",
        "        \"\"\"\n",
        "        :param config: character {A, B, C} that matches architecture in IIC supplementary materials\n",
        "        :param fan_out_init: initial fan out (paper uses 64, but can be reduced for memory constrained systems)\n",
        "        \"\"\"\n",
        "        # set activation\n",
        "        self.activation = tf.nn.relu\n",
        "\n",
        "        # save architectural details\n",
        "        self.config = config\n",
        "        self.batch_norm = batch_norm\n",
        "        self.fan_out_init = fan_out_init\n",
        "\n",
        "    def __architecture_b(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        with tf.compat.v1.variable_scope('GraphB', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # layer 1\n",
        "            num_out_channels = self.fan_out_init\n",
        "            x = convolution_layer(x=x, kernel_size=5, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1')\n",
        "            x = max_pooling_layer(x=x, pool_size=2, strides=2, name='pool1')\n",
        "\n",
        "            # layer 2\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=5, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2')\n",
        "            x = max_pooling_layer(x=x, pool_size=2, strides=2, name='pool2')\n",
        "\n",
        "            # layer 3\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=5, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3')\n",
        "            x = max_pooling_layer(x=x, pool_size=2, strides=2, name='pool3')\n",
        "\n",
        "            # layer 4\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=5, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4')\n",
        "\n",
        "            # flatten\n",
        "            x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def evaluate(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of graph\n",
        "        \"\"\"\n",
        "        # run corresponding architecture\n",
        "        if self.config == 'B':\n",
        "            return self.__architecture_b(x, is_training)\n",
        "        else:\n",
        "            raise Exception('Unknown graph configuration!')\n",
        "\n",
        "\n",
        "class VGG(object):\n",
        "    def __init__(self, config='A', batch_norm=True, fan_out_init=64):\n",
        "        \"\"\"\n",
        "        :param config: character {A, C, D} that matches architecture in VGG paper\n",
        "        :param fan_out_init: initial fan out (paper uses 64, but can be reduced for memory constrained systems)\n",
        "        \"\"\"\n",
        "        # set activation\n",
        "        self.activation = tf.nn.relu\n",
        "\n",
        "        # save architectural details\n",
        "        self.config = config\n",
        "        self.batch_norm = batch_norm\n",
        "        self.fan_out_init = fan_out_init\n",
        "\n",
        "    def __vgg_a(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        with tf.compat.v1.variable_scope('VGG_A', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # layer 1\n",
        "            num_out_channels = self.fan_out_init\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_1')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool1')\n",
        "\n",
        "            # layer 2\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_1')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool2')\n",
        "\n",
        "            # layer 3\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool3')\n",
        "\n",
        "            # layer 4\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool4')\n",
        "\n",
        "            # layer 5\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool5')\n",
        "\n",
        "            # flatten\n",
        "            x = tf.contrib.layers.flatten(x)\n",
        "\n",
        "            # fully connected layers\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc1')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc2')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc3')\n",
        "\n",
        "            return x\n",
        "\n",
        "    def __vgg_c(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        with tf.compat.v1.variable_scope('VGG_C', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "            # layer 1\n",
        "            num_out_channels = self.fan_out_init\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool1')\n",
        "\n",
        "            # layer 2\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool2')\n",
        "\n",
        "            # layer 3\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_2')\n",
        "            x = convolution_layer(x=x, kernel_size=1, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool3')\n",
        "\n",
        "            # layer 4\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_2')\n",
        "            x = convolution_layer(x=x, kernel_size=1, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool4')\n",
        "\n",
        "            # layer 5\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_2')\n",
        "            x = convolution_layer(x=x, kernel_size=1, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool5')\n",
        "\n",
        "            # flatten\n",
        "            x = tf.contrib.layers.flatten(x)\n",
        "\n",
        "            # fully connected layers\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc1')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc2')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc3')\n",
        "\n",
        "            return x\n",
        "\n",
        "    def __vgg_d(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        with tf.compat.v1.variable_scope('VGG_D', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "            # layer 1\n",
        "            num_out_channels = self.fan_out_init\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool1')\n",
        "\n",
        "            # layer 2\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool2')\n",
        "\n",
        "            # layer 3\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_2')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool3')\n",
        "\n",
        "            # layer 4\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_2')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool4')\n",
        "\n",
        "            # layer 5\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_2')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool5')\n",
        "\n",
        "            # flatten\n",
        "            x = tf.contrib.layers.flatten(x)\n",
        "\n",
        "            # fully connected layers\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc1')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc2')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc3')\n",
        "\n",
        "            return x\n",
        "\n",
        "    def evaluate(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        # run corresponding architecture\n",
        "        if self.config == 'A':\n",
        "            return self.__vgg_a(x, is_training)\n",
        "        elif self.config == 'C':\n",
        "            return self.__vgg_c(x, is_training)\n",
        "        elif self.config == 'D':\n",
        "            return self.__vgg_d(x, is_training)\n",
        "        else:\n",
        "            raise Exception('Unknown VGG configuration!')"
      ],
      "metadata": {
        "id": "-vYpL1MHgs8V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ],
      "metadata": {
        "id": "hjykHJCRg5gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "\n",
        "def save_performance(perf, epoch, save_path):\n",
        "    \"\"\"\n",
        "    :param perf: performance dictionary\n",
        "    :param epoch: epoch number\n",
        "    :param save_path: path to save plot to. if None, plot will be drawn\n",
        "    :return: none\n",
        "    \"\"\"\n",
        "    # return if save path is None\n",
        "    if save_path is None:\n",
        "        return\n",
        "\n",
        "    # loop over the metrics\n",
        "    for metric in perf.keys():\n",
        "\n",
        "        # loop over the data splits\n",
        "        for split in perf[metric].keys():\n",
        "\n",
        "            # trim data to utilized epochs\n",
        "            perf[metric][split] = perf[metric][split][:epoch]\n",
        "            assert len(perf[metric][split]) == epoch\n",
        "\n",
        "    # create the file name\n",
        "    f_name = os.path.join(save_path, 'perf.pkl')\n",
        "\n",
        "    # pickle it\n",
        "    with open(f_name, 'wb') as f:\n",
        "        pickle.dump(perf, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    # make sure it worked\n",
        "    with open(f_name, 'rb') as f:\n",
        "        perf_load = pickle.load(f)\n",
        "    assert str(perf) == str(perf_load), 'performance saving failed'\n",
        "\n",
        "\n",
        "def unsupervised_labels(y, y_hat, num_classes, num_clusters):\n",
        "    \"\"\"\n",
        "    :param y: true label\n",
        "    :param y_hat: concentration parameter\n",
        "    :param num_classes: number of classes (determined by data)\n",
        "    :param num_clusters: number of clusters (determined by model)\n",
        "    :return: classification error rate\n",
        "    \"\"\"\n",
        "    assert num_classes == num_clusters\n",
        "\n",
        "    # initialize count matrix\n",
        "    cnt_mtx = np.zeros([num_classes, num_classes])\n",
        "\n",
        "    # fill in matrix\n",
        "    for i in range(len(y)):\n",
        "        cnt_mtx[int(y_hat[i]), int(y[i])] += 1\n",
        "\n",
        "    # find optimal permutation\n",
        "    row_ind, col_ind = linear_sum_assignment(-cnt_mtx)\n",
        "\n",
        "    # compute error\n",
        "    error = 1 - cnt_mtx[row_ind, col_ind].sum() / cnt_mtx.sum()\n",
        "\n",
        "    # print results\n",
        "    print('Classification error = {:.4f}'.format(error))\n",
        "\n",
        "    return error"
      ],
      "metadata": {
        "id": "Loe1AcC7g6zk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IIC Model"
      ],
      "metadata": {
        "id": "q2piSIhdg8aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_slim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olCLUmjcriIF",
        "outputId": "721682ef-1cbc-4b45-bf9f-7c9475dd97fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf_slim in /usr/local/lib/python3.8/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from tf_slim) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches as patches\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "import tf_slim as slim\n",
        "\n",
        "# plot settings\n",
        "DPI = 600\n",
        "\n",
        "\n",
        "class ClusterIIC(object):\n",
        "    def __init__(self, num_classes, learning_rate, num_repeats, save_dir=None):\n",
        "        \"\"\"\n",
        "        :param num_classes: number of classes\n",
        "        :param learning_rate: gradient step size\n",
        "        :param num_repeats: number of data repeats for x and g(x), used to up-sample\n",
        "        \"\"\"\n",
        "        # save configuration\n",
        "        self.k_A = 5 * num_classes\n",
        "        self.num_A_sub_heads = 1\n",
        "        self.k_B = num_classes\n",
        "        self.num_B_sub_heads = 5\n",
        "        self.num_repeats = num_repeats\n",
        "\n",
        "        # initialize losses\n",
        "        self.loss_A = None\n",
        "        self.loss_B = None\n",
        "        self.losses = []\n",
        "\n",
        "        # initialize outputs\n",
        "        self.y_hats = None\n",
        "\n",
        "        # initialize optimizer\n",
        "        self.is_training = tf.compat.v1.placeholder(tf.bool)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        self.opt = tf.compat.v1.train.AdamOptimizer(self.learning_rate)\n",
        "        self.train_ops = []\n",
        "\n",
        "        # initialize performance dictionary\n",
        "        self.perf = None\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        # configure performance plotting\n",
        "        self.fig_learn, self.ax_learn = plt.subplots(1, 2)\n",
        "\n",
        "    def __iic_loss(self, pi_x, pi_gx):\n",
        "\n",
        "        # up-sample non-perturbed to match the number of repeat samples\n",
        "        pi_x = tf.tile(pi_x, [self.num_repeats] + [1] * len(pi_x.shape.as_list()[1:]))\n",
        "\n",
        "        # get K\n",
        "        k = pi_x.shape.as_list()[1]\n",
        "\n",
        "        # compute P\n",
        "        p = tf.transpose(pi_x) @ pi_gx\n",
        "\n",
        "        # enforce symmetry\n",
        "        p = (p + tf.transpose(p)) / 2\n",
        "\n",
        "        # enforce minimum value\n",
        "        p = tf.clip_by_value(p, clip_value_min=1e-6, clip_value_max=tf.float32.max)\n",
        "\n",
        "        # normalize\n",
        "        p /= tf.reduce_sum(p)\n",
        "\n",
        "        # get marginals\n",
        "        pi = tf.broadcast_to(tf.reshape(tf.reduce_sum(p, axis=0), (k, 1)), (k, k))\n",
        "        pj = tf.broadcast_to(tf.reshape(tf.reduce_sum(p, axis=1), (1, k)), (k, k))\n",
        "\n",
        "        # complete the loss\n",
        "        loss = -tf.reduce_sum(p * (tf.math.log(p) - tf.math.log(pi) - tf.math.log(pj)))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def __head_out(z, k, name):\n",
        "\n",
        "        # construct a new head that operates on the model's output for x\n",
        "        with tf.compat.v1.variable_scope(name, reuse=tf.compat.v1.AUTO_REUSE):\n",
        "            phi = tf.compat.v1.layers.dense(\n",
        "                inputs=z,\n",
        "                units=k,\n",
        "                activation=tf.nn.softmax,\n",
        "                use_bias=True,\n",
        "                kernel_initializer=KERNEL_INIT,\n",
        "                bias_initializer=BIAS_INIT)\n",
        "\n",
        "        return phi\n",
        "\n",
        "    def __head_loss(self, z_x, z_gx, k, num_sub_heads, head):\n",
        "\n",
        "        # loop over the number of sub-heads\n",
        "        loss = tf.constant(0, dtype=tf.float32)\n",
        "        for i in range(num_sub_heads):\n",
        "\n",
        "            # run the model\n",
        "            pi_x = self.__head_out(z_x, k, name=head + str(i + 1))\n",
        "            num_vars = len(tf.compat.v1.global_variables())\n",
        "            pi_gx = self.__head_out(z_gx, k, name=head + str(i + 1))\n",
        "            assert num_vars == len(tf.compat.v1.global_variables())\n",
        "\n",
        "            # accumulate the clustering loss\n",
        "            loss += self.__iic_loss(pi_x, pi_gx)\n",
        "\n",
        "        # take the average\n",
        "        if num_sub_heads > 0:\n",
        "            loss /= num_sub_heads\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def __build(self, x, gx, graph):\n",
        "\n",
        "        # run the graph\n",
        "        z_x = graph.evaluate(x, is_training=self.is_training)\n",
        "        num_vars = len(tf.compat.v1.global_variables())\n",
        "        z_gx = graph.evaluate(gx, is_training=self.is_training)\n",
        "        assert num_vars == len(tf.compat.v1.global_variables())\n",
        "\n",
        "        # construct losses\n",
        "        self.loss_A = self.__head_loss(z_x, z_gx, self.k_A, self.num_A_sub_heads, 'A')\n",
        "        self.loss_B = self.__head_loss(z_x, z_gx, self.k_B, self.num_B_sub_heads, 'B')\n",
        "        self.losses = [self.loss_A, self.loss_B]\n",
        "\n",
        "        # set alternating training operations\n",
        "        self.train_ops.append(slim.optimize_loss(loss=self.loss_A,\n",
        "                                                              global_step=self.global_step,\n",
        "                                                              learning_rate=self.learning_rate,\n",
        "                                                              optimizer=self.opt,\n",
        "                                                              summaries=['loss', 'gradients']))\n",
        "        self.train_ops.append(slim.optimize_loss(loss=self.loss_B,\n",
        "                                                              global_step=self.global_step,\n",
        "                                                              learning_rate=self.learning_rate,\n",
        "                                                              optimizer=self.opt,\n",
        "                                                              summaries=['loss', 'gradients']))\n",
        "\n",
        "        # initialize outputs outputs\n",
        "        self.y_hats = [tf.argmax(self.__head_out(z_x, self.k_B, 'B' + str(i + 1)), axis=1)\n",
        "                       for i in range(self.num_B_sub_heads)]\n",
        "\n",
        "    def __performance_dictionary_init(self, num_epochs):\n",
        "        \"\"\"\n",
        "        :param num_epochs: maximum number of epochs (used to size buffers)\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # initialize performance dictionary\n",
        "        self.perf = dict()\n",
        "\n",
        "        # loss terms\n",
        "        self.perf.update({'loss_A': np.zeros(num_epochs)})\n",
        "        self.perf.update({'loss_B': np.zeros(num_epochs)})\n",
        "\n",
        "        # classification error\n",
        "        self.perf.update({'class_err_min': np.zeros(num_epochs)})\n",
        "        self.perf.update({'class_err_avg': np.zeros(num_epochs)})\n",
        "        self.perf.update({'class_err_max': np.zeros(num_epochs)})\n",
        "\n",
        "    def __classification_accuracy(self, sess, iter_init, idx, y_ph=None):\n",
        "        \"\"\"\n",
        "        :param sess: TensorFlow session\n",
        "        :param iter_init: TensorFlow data iterator initializer associated\n",
        "        :param idx: insertion index (i.e. epoch - 1)\n",
        "        :param y_ph: TensorFlow placeholder for unseen labels\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        if self.perf is None or y_ph is None:\n",
        "            return\n",
        "\n",
        "        # initialize results\n",
        "        y = np.zeros([0, 1])\n",
        "        y_hats = [np.zeros([0, 1])] * self.num_B_sub_heads\n",
        "\n",
        "        # initialize unsupervised data iterator\n",
        "        sess.run(iter_init)\n",
        "\n",
        "        # loop over the batches within the unsupervised data iterator\n",
        "        print('Evaluating classification accuracy... ')\n",
        "        while True:\n",
        "            try:\n",
        "                # grab the results\n",
        "                results = sess.run([self.y_hats, y_ph], feed_dict={self.is_training: False})\n",
        "\n",
        "                # load metrics\n",
        "                for i in range(self.num_B_sub_heads):\n",
        "                    y_hats[i] = np.concatenate((y_hats[i], np.expand_dims(results[0][i], axis=1)))\n",
        "                if y_ph is not None:\n",
        "                    y = np.concatenate((y, np.expand_dims(results[1], axis=1)))\n",
        "\n",
        "                # _, ax = plt.subplots(2, 10)\n",
        "                # i_rand = np.random.choice(results[3].shape[0], 10)\n",
        "                # for i in range(10):\n",
        "                #     ax[0, i].imshow(results[3][i_rand[i]][:, :, 0], origin='upper', vmin=0, vmax=1)\n",
        "                #     ax[0, i].set_xticks([])\n",
        "                #     ax[0, i].set_yticks([])\n",
        "                #     ax[1, i].imshow(results[4][i_rand[i]][:, :, 0], origin='upper', vmin=0, vmax=1)\n",
        "                #     ax[1, i].set_xticks([])\n",
        "                #     ax[1, i].set_yticks([])\n",
        "                # plt.show()\n",
        "\n",
        "            # iterator will throw this error when its out of data\n",
        "            except tf.errors.OutOfRangeError:\n",
        "                break\n",
        "\n",
        "        # compute classification accuracy\n",
        "        if y_ph is not None:\n",
        "            class_errors = [unsupervised_labels(y, y_hats[i], self.k_B, self.k_B)\n",
        "                            for i in range(self.num_B_sub_heads)]\n",
        "            self.perf['class_err_min'][idx] = np.min(class_errors)\n",
        "            self.perf['class_err_avg'][idx] = np.mean(class_errors)\n",
        "            self.perf['class_err_max'][idx] = np.max(class_errors)\n",
        "\n",
        "        # metrics are done\n",
        "        print('Done')\n",
        "\n",
        "    def plot_learning_curve(self, epoch):\n",
        "        \"\"\"\n",
        "        :param epoch: epoch number\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # generate epoch numbers\n",
        "        t = np.arange(1, epoch + 1)\n",
        "\n",
        "        # colors\n",
        "        c = {'Head A': '#1f77b4', 'Head B': '#ff7f0e'}\n",
        "\n",
        "        # plot the loss\n",
        "        self.ax_learn[0].clear()\n",
        "        self.ax_learn[0].set_title('Loss')\n",
        "        self.ax_learn[0].plot(t, self.perf['loss_A'][:epoch], label='Head A', color=c['Head A'])\n",
        "        self.ax_learn[0].plot(t, self.perf['loss_B'][:epoch], label='Head B', color=c['Head B'])\n",
        "        self.ax_learn[0].xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        self.ax_learn[0].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "\n",
        "        # plot the classification error\n",
        "        self.ax_learn[1].clear()\n",
        "        self.ax_learn[1].set_title('Class. Error (Min, Avg, Max)')\n",
        "        self.ax_learn[1].plot(t, self.perf['class_err_avg'][:epoch], color=c['Head B'])\n",
        "        self.ax_learn[1].fill_between(t,\n",
        "                                      self.perf['class_err_min'][:epoch],\n",
        "                                      self.perf['class_err_max'][:epoch],\n",
        "                                      facecolor=c['Head B'], alpha=0.5)\n",
        "        self.ax_learn[1].plot(t, self.perf['class_err_avg'][:epoch], color=c['Head B'])\n",
        "        self.ax_learn[1].fill_between(t,\n",
        "                                      self.perf['class_err_min'][:epoch],\n",
        "                                      self.perf['class_err_max'][:epoch],\n",
        "                                      facecolor=c['Head B'], alpha=0.5)\n",
        "        self.ax_learn[1].xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        self.ax_learn[1].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "\n",
        "        # make the legend\n",
        "        self.ax_learn[1].legend(handles=[patches.Patch(color=val, label=key) for key, val in c.items()],\n",
        "                                ncol=len(c),\n",
        "                                bbox_to_anchor=(0.35, -0.06))\n",
        "\n",
        "        # eliminate those pesky margins\n",
        "        self.fig_learn.subplots_adjust(left=0.1, bottom=0.15, right=0.95, top=0.95, wspace=0.25, hspace=0.3)\n",
        "\n",
        "    def train(self, graph, train_set, test_set, num_epochs, early_stop_buffer=15):\n",
        "        \"\"\"\n",
        "        :param graph: the computational graph\n",
        "        :param train_set: TensorFlow Dataset object that corresponds to training data\n",
        "        :param test_set: TensorFlow Dataset object that corresponds to validation data\n",
        "        :param num_epochs: number of epochs\n",
        "        :param early_stop_buffer: early stop look-ahead distance (in epochs)\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # construct iterator\n",
        "        iterator = tf.compat.v1.data.make_initializable_iterator(train_set)\n",
        "        x, gx, y = iterator.get_next().values()\n",
        "\n",
        "        # construct initialization operations\n",
        "        train_iter_init = iterator.make_initializer(train_set)\n",
        "        test_iter_init = iterator.make_initializer(test_set)\n",
        "\n",
        "        # build the model using the supplied computational graph\n",
        "        self.__build(x, gx, graph)\n",
        "\n",
        "        # initialize performance dictionary\n",
        "        self.__performance_dictionary_init(num_epochs)\n",
        "\n",
        "        # start a monitored session\n",
        "        cfg = tf.compat.v1.ConfigProto()\n",
        "        cfg.gpu_options.allow_growth = True\n",
        "        with tf.compat.v1.Session(config=cfg) as sess:\n",
        "\n",
        "            # initialize model variables\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "            # loop over the number of epochs\n",
        "            for i in range(num_epochs):\n",
        "\n",
        "                # start timer\n",
        "                start = time.time()\n",
        "\n",
        "                # get epoch number\n",
        "                epoch = i + 1\n",
        "\n",
        "                # get training operation\n",
        "                i_train = i % len(self.train_ops)\n",
        "\n",
        "                # initialize epoch iterator\n",
        "                sess.run(train_iter_init)\n",
        "\n",
        "                # loop over the batches\n",
        "                loss_A = []\n",
        "                loss_B = []\n",
        "                while True:\n",
        "                    try:\n",
        "\n",
        "                        # run training and losses\n",
        "                        losses = sess.run([self.train_ops[i_train]] + [self.losses],\n",
        "                                          feed_dict={self.is_training: True})[-1]\n",
        "\n",
        "                        # load metrics\n",
        "                        loss_A.append(losses[0])\n",
        "                        loss_B.append(losses[1])\n",
        "\n",
        "                        if np.isnan(losses).any():\n",
        "                            print('\\n NaN whelp!')\n",
        "                            return\n",
        "\n",
        "                        # print update\n",
        "                        print('\\rEpoch {:d}, Loss = {:.4f}'.format(epoch, losses[i_train]), end='')\n",
        "\n",
        "                    # iterator will throw this error when its out of data\n",
        "                    except tf.errors.OutOfRangeError:\n",
        "                        break\n",
        "\n",
        "                # new line\n",
        "                print('')\n",
        "\n",
        "                # save averaged training performance\n",
        "                self.perf['loss_A'][i] = np.mean(loss_A)\n",
        "                self.perf['loss_B'][i] = np.mean(loss_B)\n",
        "\n",
        "                # get classification performance\n",
        "                self.__classification_accuracy(sess, test_iter_init, i, y)\n",
        "\n",
        "                # plot learning curve\n",
        "                self.plot_learning_curve(epoch)\n",
        "                # pause for plot drawing if we aren't saving\n",
        "                if self.save_dir is None:\n",
        "                    plt.pause(0.05)\n",
        "\n",
        "                # print time for epoch\n",
        "                stop = time.time()\n",
        "                print('Time for Epoch = {:f}'.format(stop - start))\n",
        "\n",
        "                # early stop check\n",
        "                # i_best_elbo = np.argmin(self.perf['loss']['test'][:epoch])\n",
        "                # i_best_class = np.argmin(self.perf['class_err']['test'][:epoch])\n",
        "                # epochs_since_improvement = min(i - i_best_elbo, i - i_best_class)\n",
        "                # print('Early stop checks: {:d} / {:d}\\n'.format(epochs_since_improvement, early_stop_buffer))\n",
        "                # if epochs_since_improvement >= early_stop_buffer:\n",
        "                #     break\n",
        "\n",
        "        # save the performance\n",
        "        save_performance(self.perf, epoch, self.save_dir)"
      ],
      "metadata": {
        "id": "AULCcYxZgY3C"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Running"
      ],
      "metadata": {
        "id": "_-Hevd1ShMl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "-ZIKkkrCmgwy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a data set\n",
        "DATA_SET = 'mnist'\n",
        "\n",
        "# define splits\n",
        "DS_CONFIG = {\n",
        "    # mnist data set parameters\n",
        "    'mnist': {\n",
        "        'batch_size': 700,\n",
        "        'num_repeats': 5,\n",
        "        'mdl_input_dims': [24, 24, 1]}\n",
        "}\n",
        "\n",
        "# load the data set\n",
        "TRAIN_SET, TEST_SET, SET_INFO = load(data_set_name=DATA_SET, **DS_CONFIG[DATA_SET])\n",
        "\n",
        "# configure the common model elements\n",
        "MDL_CONFIG = {\n",
        "    # mist hyper-parameters\n",
        "    'mnist': {\n",
        "        'num_classes': SET_INFO.features['label'].num_classes,\n",
        "        'learning_rate': 1e-4,\n",
        "        'num_repeats': DS_CONFIG[DATA_SET]['num_repeats'],\n",
        "        'save_dir': None},\n",
        "}\n",
        "\n",
        "# declare the model\n",
        "mdl = ClusterIIC(**MDL_CONFIG[DATA_SET])\n",
        "\n",
        "# train the model\n",
        "mdl.train(IICGraph(config='B', batch_norm=True, fan_out_init=64), TRAIN_SET, TEST_SET, num_epochs=50)\n",
        "\n",
        "print('All done!')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UX3pdaYdgdOw",
        "outputId": "731583db-0cff-4ac4-a0e4-dc867ff8a65b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-5e91a061e0d9>:21: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  x = tf.compat.v1.layers.conv2d(inputs=x,\n",
            "<ipython-input-11-5e91a061e0d9>:51: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  x = tf.compat.v1.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=strides, padding='same', name=name)\n",
            "<ipython-input-14-bb9762daad69>:84: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  phi = tf.compat.v1.layers.dense(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss = -0.5193\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.8875\n",
            "Classification error = 0.8271\n",
            "Classification error = 0.7281\n",
            "Classification error = 0.7626\n",
            "Classification error = 0.7647\n",
            "Done\n",
            "[-0.25930542  0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for Epoch = 88.597301\n",
            "Epoch 2, Loss = -0.5690\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.7404\n",
            "Classification error = 0.6753\n",
            "Classification error = 0.7080\n",
            "Classification error = 0.6974\n",
            "Classification error = 0.6924\n",
            "Done\n",
            "[-0.25930542 -0.37527761  0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 92.642069\n",
            "Epoch 3, Loss = -0.9799\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.7317\n",
            "Classification error = 0.6693\n",
            "Classification error = 0.7538\n",
            "Classification error = 0.6737\n",
            "Classification error = 0.7036\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845  0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.986288\n",
            "Epoch 4, Loss = -0.8123\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.6105\n",
            "Classification error = 0.5870\n",
            "Classification error = 0.7001\n",
            "Classification error = 0.5592\n",
            "Classification error = 0.5874\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804  0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 67.113713\n",
            "Epoch 5, Loss = -0.8325\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.6262\n",
            "Classification error = 0.6002\n",
            "Classification error = 0.6999\n",
            "Classification error = 0.5645\n",
            "Classification error = 0.5959\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848  0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.127440\n",
            "Epoch 6, Loss = -0.9951\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.5705\n",
            "Classification error = 0.5381\n",
            "Classification error = 0.6939\n",
            "Classification error = 0.5397\n",
            "Classification error = 0.5801\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.687073\n",
            "Epoch 7, Loss = -1.1076\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.5668\n",
            "Classification error = 0.5263\n",
            "Classification error = 0.6702\n",
            "Classification error = 0.5206\n",
            "Classification error = 0.5866\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815  0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 67.307707\n",
            "Epoch 8, Loss = -0.8917\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.5156\n",
            "Classification error = 0.4798\n",
            "Classification error = 0.6269\n",
            "Classification error = 0.4910\n",
            "Classification error = 0.5136\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959  0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 67.730600\n",
            "Epoch 9, Loss = -1.3780\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.5172\n",
            "Classification error = 0.4686\n",
            "Classification error = 0.6288\n",
            "Classification error = 0.4936\n",
            "Classification error = 0.4885\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071   0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.768255\n",
            "Epoch 10, Loss = -1.3026\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.4653\n",
            "Classification error = 0.3736\n",
            "Classification error = 0.5156\n",
            "Classification error = 0.4063\n",
            "Classification error = 0.3639\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522  0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.762226\n",
            "Epoch 11, Loss = -1.4345\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.4554\n",
            "Classification error = 0.4026\n",
            "Classification error = 0.5128\n",
            "Classification error = 0.4136\n",
            "Classification error = 0.3633\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249  0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 67.311250\n",
            "Epoch 12, Loss = -1.3359\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.4349\n",
            "Classification error = 0.2907\n",
            "Classification error = 0.5097\n",
            "Classification error = 0.3457\n",
            "Classification error = 0.2971\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.545101\n",
            "Epoch 13, Loss = -1.5946\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.4160\n",
            "Classification error = 0.2511\n",
            "Classification error = 0.4988\n",
            "Classification error = 0.3529\n",
            "Classification error = 0.2781\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533  0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 63.048719\n",
            "Epoch 14, Loss = -1.4525\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3869\n",
            "Classification error = 0.1830\n",
            "Classification error = 0.4749\n",
            "Classification error = 0.3333\n",
            "Classification error = 0.2504\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471  0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 91.445901\n",
            "Epoch 15, Loss = -1.7747\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3880\n",
            "Classification error = 0.1841\n",
            "Classification error = 0.4777\n",
            "Classification error = 0.3326\n",
            "Classification error = 0.2467\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192  0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 67.301228\n",
            "Epoch 16, Loss = -1.3775\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3785\n",
            "Classification error = 0.1779\n",
            "Classification error = 0.4712\n",
            "Classification error = 0.2790\n",
            "Classification error = 0.2270\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791  0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.629582\n",
            "Epoch 17, Loss = -1.8629\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3817\n",
            "Classification error = 0.1889\n",
            "Classification error = 0.4680\n",
            "Classification error = 0.2720\n",
            "Classification error = 0.2238\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552  0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 92.065973\n",
            "Epoch 18, Loss = -1.3866\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3768\n",
            "Classification error = 0.1600\n",
            "Classification error = 0.3859\n",
            "Classification error = 0.2306\n",
            "Classification error = 0.1610\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 63.399301\n",
            "Epoch 19, Loss = -2.0257\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3763\n",
            "Classification error = 0.1652\n",
            "Classification error = 0.3968\n",
            "Classification error = 0.2379\n",
            "Classification error = 0.1741\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956  0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 66.875673\n",
            "Epoch 20, Loss = -1.5920\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3712\n",
            "Classification error = 0.1417\n",
            "Classification error = 0.3125\n",
            "Classification error = 0.2269\n",
            "Classification error = 0.1370\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213  0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.885890\n",
            "Epoch 21, Loss = -2.1732\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3754\n",
            "Classification error = 0.1597\n",
            "Classification error = 0.3295\n",
            "Classification error = 0.2381\n",
            "Classification error = 0.1539\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362  0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 92.182488\n",
            "Epoch 22, Loss = -1.7667\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3684\n",
            "Classification error = 0.1451\n",
            "Classification error = 0.3109\n",
            "Classification error = 0.2212\n",
            "Classification error = 0.1314\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548  0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 90.775871\n",
            "Epoch 23, Loss = -2.1038\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3755\n",
            "Classification error = 0.1619\n",
            "Classification error = 0.3267\n",
            "Classification error = 0.2301\n",
            "Classification error = 0.1401\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556  0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 68.907781\n",
            "Epoch 24, Loss = -1.4249\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3679\n",
            "Classification error = 0.1386\n",
            "Classification error = 0.3146\n",
            "Classification error = 0.2192\n",
            "Classification error = 0.1299\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 63.515587\n",
            "Epoch 25, Loss = -2.2411\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3710\n",
            "Classification error = 0.1513\n",
            "Classification error = 0.3251\n",
            "Classification error = 0.2247\n",
            "Classification error = 0.1330\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513   0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 66.082530\n",
            "Epoch 26, Loss = -1.7383\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3621\n",
            "Classification error = 0.1406\n",
            "Classification error = 0.3149\n",
            "Classification error = 0.2200\n",
            "Classification error = 0.1295\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961  0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 63.644150\n",
            "Epoch 27, Loss = -2.3694\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3686\n",
            "Classification error = 0.1580\n",
            "Classification error = 0.3136\n",
            "Classification error = 0.2253\n",
            "Classification error = 0.1365\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429  0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.267614\n",
            "Epoch 28, Loss = -1.8562\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3629\n",
            "Classification error = 0.1364\n",
            "Classification error = 0.3165\n",
            "Classification error = 0.2188\n",
            "Classification error = 0.1286\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422  0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.894047\n",
            "Epoch 29, Loss = -1.9744\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3716\n",
            "Classification error = 0.1502\n",
            "Classification error = 0.3149\n",
            "Classification error = 0.2249\n",
            "Classification error = 0.1299\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805  0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.752971\n",
            "Epoch 30, Loss = -1.3559\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3564\n",
            "Classification error = 0.1398\n",
            "Classification error = 0.3130\n",
            "Classification error = 0.2194\n",
            "Classification error = 0.1283\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 63.355175\n",
            "Epoch 31, Loss = -2.0652\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3591\n",
            "Classification error = 0.1568\n",
            "Classification error = 0.3229\n",
            "Classification error = 0.2241\n",
            "Classification error = 0.1339\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275   0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.755837\n",
            "Epoch 32, Loss = -1.4533\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3624\n",
            "Classification error = 0.1378\n",
            "Classification error = 0.3127\n",
            "Classification error = 0.2196\n",
            "Classification error = 0.1281\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354  0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 67.598054\n",
            "Epoch 33, Loss = -2.5942\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3702\n",
            "Classification error = 0.1620\n",
            "Classification error = 0.3076\n",
            "Classification error = 0.2238\n",
            "Classification error = 0.1362\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814  0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.676008\n",
            "Epoch 34, Loss = -1.6273\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3643\n",
            "Classification error = 0.1366\n",
            "Classification error = 0.3136\n",
            "Classification error = 0.2162\n",
            "Classification error = 0.1271\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246  0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.798822\n",
            "Epoch 35, Loss = -2.3470\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3679\n",
            "Classification error = 0.1425\n",
            "Classification error = 0.3029\n",
            "Classification error = 0.2196\n",
            "Classification error = 0.1286\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114  0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.817088\n",
            "Epoch 36, Loss = -1.6960\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3646\n",
            "Classification error = 0.1357\n",
            "Classification error = 0.3107\n",
            "Classification error = 0.2157\n",
            "Classification error = 0.1268\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 92.414536\n",
            "Epoch 37, Loss = -2.5604\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3667\n",
            "Classification error = 0.1375\n",
            "Classification error = 0.3080\n",
            "Classification error = 0.2189\n",
            "Classification error = 0.1297\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888  0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.718202\n",
            "Epoch 38, Loss = -1.7910\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3512\n",
            "Classification error = 0.1346\n",
            "Classification error = 0.3162\n",
            "Classification error = 0.2158\n",
            "Classification error = 0.1257\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929  0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 67.664113\n",
            "Epoch 39, Loss = -2.4976\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3589\n",
            "Classification error = 0.1386\n",
            "Classification error = 0.3113\n",
            "Classification error = 0.2193\n",
            "Classification error = 0.1300\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027   0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 61.824870\n",
            "Epoch 40, Loss = -1.7277\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3587\n",
            "Classification error = 0.1360\n",
            "Classification error = 0.3092\n",
            "Classification error = 0.2157\n",
            "Classification error = 0.1287\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026  0.          0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.708921\n",
            "Epoch 41, Loss = -2.4042\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3555\n",
            "Classification error = 0.1476\n",
            "Classification error = 0.3067\n",
            "Classification error = 0.2185\n",
            "Classification error = 0.1271\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931  0.\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 92.972203\n",
            "Epoch 42, Loss = -1.6953\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3507\n",
            "Classification error = 0.1384\n",
            "Classification error = 0.3110\n",
            "Classification error = 0.2164\n",
            "Classification error = 0.1272\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931 -2.4707489\n",
            "  0.          0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.770622\n",
            "Epoch 43, Loss = -2.3390\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3516\n",
            "Classification error = 0.1451\n",
            "Classification error = 0.3021\n",
            "Classification error = 0.2167\n",
            "Classification error = 0.1320\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931 -2.4707489\n",
            " -2.47953796  0.          0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 87.664700\n",
            "Epoch 44, Loss = -1.5085\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3612\n",
            "Classification error = 0.1364\n",
            "Classification error = 0.3074\n",
            "Classification error = 0.2150\n",
            "Classification error = 0.1253\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931 -2.4707489\n",
            " -2.47953796 -2.45924234  0.          0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 63.140770\n",
            "Epoch 45, Loss = -2.5640\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3635\n",
            "Classification error = 0.1445\n",
            "Classification error = 0.3117\n",
            "Classification error = 0.2185\n",
            "Classification error = 0.1292\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931 -2.4707489\n",
            " -2.47953796 -2.45924234 -2.51679039  0.          0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 66.675655\n",
            "Epoch 46, Loss = -1.7792\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3580\n",
            "Classification error = 0.1334\n",
            "Classification error = 0.3144\n",
            "Classification error = 0.2149\n",
            "Classification error = 0.1252\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931 -2.4707489\n",
            " -2.47953796 -2.45924234 -2.51679039 -2.51149392  0.          0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 77.039465\n",
            "Epoch 47, Loss = -2.6536\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3607\n",
            "Classification error = 0.1378\n",
            "Classification error = 0.3156\n",
            "Classification error = 0.2169\n",
            "Classification error = 0.1269\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931 -2.4707489\n",
            " -2.47953796 -2.45924234 -2.51679039 -2.51149392 -2.52225542  0.\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 64.545329\n",
            "Epoch 48, Loss = -1.9304\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3473\n",
            "Classification error = 0.1387\n",
            "Classification error = 0.3110\n",
            "Classification error = 0.2148\n",
            "Classification error = 0.1256\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931 -2.4707489\n",
            " -2.47953796 -2.45924234 -2.51679039 -2.51149392 -2.52225542 -2.53240657\n",
            "  0.          0.        ]\n",
            "Time for Epoch = 62.566001\n",
            "Epoch 49, Loss = -2.5259\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3492\n",
            "Classification error = 0.1477\n",
            "Classification error = 0.3050\n",
            "Classification error = 0.2215\n",
            "Classification error = 0.1328\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931 -2.4707489\n",
            " -2.47953796 -2.45924234 -2.51679039 -2.51149392 -2.52225542 -2.53240657\n",
            " -2.54915142  0.        ]\n",
            "Time for Epoch = 87.559265\n",
            "Epoch 50, Loss = -1.7996\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.3506\n",
            "Classification error = 0.1371\n",
            "Classification error = 0.3107\n",
            "Classification error = 0.2132\n",
            "Classification error = 0.1240\n",
            "Done\n",
            "[-0.25930542 -0.37527761 -0.71571845 -0.70913804 -0.94098848 -1.0532788\n",
            " -1.17972815 -1.24925959 -1.3445071  -1.36998522 -1.50561249 -1.46932375\n",
            " -1.70617533 -1.65612471 -1.82936192 -1.82394791 -1.92568552 -1.93143702\n",
            " -1.96324956 -1.97515213 -2.05867362 -2.05038548 -2.11066556 -2.08745074\n",
            " -2.2028513  -2.18878961 -2.23771429 -2.21536422 -2.28279805 -2.26452875\n",
            " -2.2936275  -2.28579354 -2.35378814 -2.30027246 -2.38251114 -2.37574553\n",
            " -2.40770888 -2.41976929 -2.4581027  -2.42242026 -2.46999931 -2.4707489\n",
            " -2.47953796 -2.45924234 -2.51679039 -2.51149392 -2.52225542 -2.53240657\n",
            " -2.54915142 -2.4988873 ]\n",
            "Time for Epoch = 68.125456\n",
            "All done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1,2)\n",
        "epoch = 50\n",
        "t = np.arange(1, epoch + 1)\n",
        "\n",
        "# colors\n",
        "c = {'Head A': '#1f77b4', 'Head B': '#ff7f0e'}\n",
        "\n",
        "# plot the loss\n",
        "axes[0].clear()\n",
        "axes[0].set_title('Loss')\n",
        "axes[0].plot(t, mdl.perf['loss_A'][:epoch], label='Head A', color=c['Head A'])\n",
        "axes[0].plot(t, mdl.perf['loss_B'][:epoch], label='Head B', color=c['Head B'])\n",
        "axes[0].xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "axes[0].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "\n",
        "axes[1].set_title('Class. Error (Min, Avg, Max)')\n",
        "axes[1].plot(t, mdl.perf['class_err_avg'][:epoch], color=c['Head B'])\n",
        "axes[1].fill_between(t,\n",
        "                              mdl.perf['class_err_min'][:epoch],\n",
        "                              mdl.perf['class_err_max'][:epoch],\n",
        "                              facecolor=c['Head B'], alpha=0.5)\n",
        "axes[1].plot(t, mdl.perf['class_err_avg'][:epoch], color=c['Head B'])\n",
        "axes[1].fill_between(t,\n",
        "                              mdl.perf['class_err_min'][:epoch],\n",
        "                              mdl.perf['class_err_max'][:epoch],\n",
        "                              facecolor=c['Head B'], alpha=0.5)\n",
        "axes[1].xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "axes[1].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "9aZv3DXEdCvb",
        "outputId": "d57b590f-11f2-4946-8615-ba5adb186c48"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third try : IIC on our dataset"
      ],
      "metadata": {
        "id": "fwQ9lFrzCtde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data"
      ],
      "metadata": {
        "id": "lWaPrDiuFvhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFhYr2tuPIJm",
        "outputId": "77e1fe6b-6bcd-438e-a42e-f462fb01125a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.8/dist-packages (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "def mnist_x(x_orig, mdl_input_dims, is_training):\n",
        "\n",
        "    # rescale to [0, 1]\n",
        "    x_orig = tf.cast(x_orig, dtype=tf.float32) / x_orig.dtype.max\n",
        "\n",
        "    # get common shapes\n",
        "    height_width = mdl_input_dims[:-1]\n",
        "    n_chans = mdl_input_dims[-1]\n",
        "\n",
        "    # training transformations\n",
        "    if is_training:\n",
        "        x1 = tf.image.central_crop(x_orig, np.mean(20 / np.array(x_orig.shape.as_list()[1:-1])))\n",
        "        x2 = tf.image.random_crop(x_orig, tf.concat((tf.shape(x_orig)[:1], [20, 20], [n_chans]), axis=0))\n",
        "        x = tf.stack([x1, x2])\n",
        "        x = tf.transpose(x, [1, 0, 2, 3, 4])\n",
        "        i = tf.squeeze(tf.random.categorical([[1., 1.]], tf.shape(x)[0]))\n",
        "        x = tf.map_fn(lambda y: y[0][y[1]], (x, i), dtype=tf.float32)\n",
        "        x = tf.image.resize(x, height_width)\n",
        "\n",
        "    # testing transformations\n",
        "    else:\n",
        "        x = tf.image.central_crop(x_orig, np.mean(20 / np.array(x_orig.shape.as_list()[1:-1])))\n",
        "        x = tf.image.resize(x, height_width)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def mnist_gx(x_orig, mdl_input_dims, is_training, sample_repeats):\n",
        "\n",
        "    # if not training, return a constant value--it will unused but needs to be same shape to avoid TensorFlow errors\n",
        "    if not is_training:\n",
        "        return tf.zeros([0] + mdl_input_dims)\n",
        "\n",
        "    # rescale to [0, 1]\n",
        "    x_orig = tf.cast(x_orig, dtype=tf.float32) / x_orig.dtype.max\n",
        "\n",
        "    # repeat samples accordingly\n",
        "    x_orig = tf.tile(x_orig, [sample_repeats] + [1] * len(x_orig.shape.as_list()[1:]))\n",
        "\n",
        "    # get common shapes\n",
        "    height_width = mdl_input_dims[:-1]\n",
        "    n_chans = mdl_input_dims[-1]\n",
        "\n",
        "    # random rotation\n",
        "    rad = 2 * np.pi * 25 / 360\n",
        "    x_rot = tfa.image.rotate(x_orig, tf.random.uniform(shape=tf.shape(x_orig)[:1], minval=-rad, maxval=rad))\n",
        "    gx = tf.stack([x_orig, x_rot])\n",
        "    gx = tf.transpose(gx, [1, 0, 2, 3, 4])\n",
        "    i = tf.squeeze(tf.random.categorical([[1., 1.]], tf.shape(gx)[0]))\n",
        "    gx = tf.map_fn(lambda y: y[0][y[1]], (gx, i), dtype=tf.float32)\n",
        "\n",
        "    # random crops\n",
        "    x1 = tf.image.random_crop(gx, tf.concat((tf.shape(x_orig)[:1], [16, 16], [n_chans]), axis=0))\n",
        "    x2 = tf.image.random_crop(gx, tf.concat((tf.shape(x_orig)[:1], [20, 20], [n_chans]), axis=0))\n",
        "    x3 = tf.image.random_crop(gx, tf.concat((tf.shape(x_orig)[:1], [24, 24], [n_chans]), axis=0))\n",
        "    gx = tf.stack([tf.image.resize(x1, height_width),\n",
        "                   tf.image.resize(x2, height_width),\n",
        "                   tf.image.resize(x3, height_width)])\n",
        "    gx = tf.transpose(gx, [1, 0, 2, 3, 4])\n",
        "    i = tf.squeeze(tf.random.categorical([[1., 1., 1.]], tf.shape(gx)[0]))\n",
        "    gx = tf.map_fn(lambda y: y[0][y[1]], (gx, i), dtype=tf.float32)\n",
        "\n",
        "    # apply random adjustments\n",
        "    def rand_adjust(img):\n",
        "        img = tf.image.random_brightness(img, 0.4)\n",
        "        img = tf.image.random_contrast(img, 0.6, 1.4)\n",
        "        if img.shape.as_list()[-1] == 3:\n",
        "            img = tf.image.random_saturation(img, 0.6, 1.4)\n",
        "            img = tf.image.random_hue(img, 0.125)\n",
        "        return img\n",
        "\n",
        "    gx = tf.map_fn(lambda y: rand_adjust(y), gx, dtype=tf.float32)\n",
        "\n",
        "    return gx\n",
        "\n",
        "\n",
        "def pre_process_data(ds, info, is_training, **kwargs):\n",
        "    \"\"\"\n",
        "    :param ds: TensorFlow Dataset object\n",
        "    :param info: TensorFlow DatasetInfo object\n",
        "    :param is_training: indicator to pre-processing function\n",
        "    :return: the passed in data set with map pre-processing applied\n",
        "    \"\"\"\n",
        "    # apply pre-processing function for given data set and run-time conditions\n",
        "    if info == 'german_signs':\n",
        "        return ds.map(lambda d: {'x': mnist_x(d['image'],\n",
        "                                              mdl_input_dims=kwargs['mdl_input_dims'],\n",
        "                                              is_training=is_training),\n",
        "                                       'gx': mnist_gx(d['image'],\n",
        "                                                mdl_input_dims=kwargs['mdl_input_dims'],\n",
        "                                                is_training=is_training,\n",
        "                                                sample_repeats=kwargs['num_repeats']),\n",
        "                                       'label': d['label']},\n",
        "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    else:\n",
        "        raise Exception('Unsupported data set: ' + info.name)\n",
        "\n",
        "\n",
        "def configure_data_set(ds, info, batch_size, is_training, **kwargs):\n",
        "    \"\"\"\n",
        "    :param ds: TensorFlow data set object\n",
        "    :param info: TensorFlow DatasetInfo object\n",
        "    :param batch_size: batch size\n",
        "    :param is_training: indicator to pre-processing function\n",
        "    :return: a configured TensorFlow data set object\n",
        "    \"\"\"\n",
        "    # enable shuffling and repeats\n",
        "    ds = ds.shuffle(10 * batch_size, reshuffle_each_iteration=True).repeat(1)\n",
        "\n",
        "    # batch the data before pre-processing\n",
        "    ds = ds.batch(batch_size)\n",
        "\n",
        "    # pre-process the data set\n",
        "    with tf.device('/cpu:0'):\n",
        "        ds = pre_process_data(ds, info, is_training, **kwargs)\n",
        "\n",
        "    # enable prefetch\n",
        "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "def load(X_train, y_train, X_test, y_test, **kwargs):\n",
        "    \"\"\"\n",
        "    :param data_set_name: data set name--call tfds.list_builders() for options\n",
        "    :return:\n",
        "        train_dataset: TensorFlow Dataset object for the training data\n",
        "        test_dataset: TensorFlow Dataset object for the testing data\n",
        "        info: data set info object\n",
        "    \"\"\"\n",
        "    # get data and its info\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices({'image' : X_train, 'label' : y_train})\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices({'image' : X_test, 'label' : y_test})\n",
        "\n",
        "    # configure the data sets\n",
        " \n",
        "    train_ds = configure_data_set(ds=train_dataset, info='german_signs', is_training=True, **kwargs)\n",
        "\n",
        "    test_ds = configure_data_set(ds=test_dataset, info='german_signs', is_training=False, **kwargs)\n",
        "\n",
        "\n",
        "    return train_ds, test_ds, 'german_signs'"
      ],
      "metadata": {
        "id": "2TXxsh0uFuyl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Graph"
      ],
      "metadata": {
        "id": "a0l7RC3dcsvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# set trainable variable initialization routines\n",
        "KERNEL_INIT = tf.keras.initializers.he_uniform()\n",
        "WEIGHT_INIT = tf.random_normal_initializer(mean=0.0, stddev=0.01)\n",
        "BIAS_INIT = tf.constant_initializer(0.0)\n",
        "\n",
        "\n",
        "def convolution_layer(x, kernel_size, num_out_channels, activation, batch_norm, is_training, name):\n",
        "    \"\"\"\n",
        "    :param x: input data\n",
        "    :param kernel_size: convolution kernel size\n",
        "    :param num_out_channels: number of output channels\n",
        "    :param activation: non-linearity\n",
        "    :param batch_norm: whether to use batch norm\n",
        "    :param is_training: whether we are training or testing (used by batch normalization)\n",
        "    :param name: variable scope name (empowers variable reuse)\n",
        "    :return: layer output\n",
        "    \"\"\"\n",
        "    # run convolution layer\n",
        "    x = tf.compat.v1.layers.conv2d(inputs=x,\n",
        "                         filters=num_out_channels,\n",
        "                         kernel_size=[kernel_size] * 2,\n",
        "                         strides=[1, 1],\n",
        "                         padding='same',\n",
        "                         activation=None,\n",
        "                         use_bias=True,\n",
        "                         kernel_initializer=KERNEL_INIT,\n",
        "                         bias_initializer=BIAS_INIT,\n",
        "                         name=name)\n",
        "    # run batch norm if specified\n",
        "    if batch_norm:\n",
        "        x = tf.compat.v1.layers.BatchNormalization()(x)\n",
        "\n",
        "    # run activation\n",
        "    x = activation(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def max_pooling_layer(x, pool_size, strides, name):\n",
        "    \"\"\"\n",
        "    :param x: input data\n",
        "    :param pool_size: pooling kernel size\n",
        "    :param strides: pooling stride length\n",
        "    :param name: variable scope name (empowers variable reuse)\n",
        "    :return: layer output\n",
        "    \"\"\"\n",
        "    # run max pooling\n",
        "    x = tf.compat.v1.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=strides, padding='same', name=name)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def fully_connected_layer(x, num_outputs, activation, is_training, name):\n",
        "    \"\"\"\n",
        "    :param x: input data\n",
        "    :param num_outputs: number of outputs\n",
        "    :param activation: non-linearity\n",
        "    :param is_training: whether we are training or testing (used by batch normalization)\n",
        "    :param name: variable scope name (empowers variable reuse)\n",
        "    :return: layer output\n",
        "    \"\"\"\n",
        "    # run dense layer\n",
        "    x = tf.compat.v1.layers.dense(inputs=x,\n",
        "                        units=num_outputs,\n",
        "                        activation=None,\n",
        "                        use_bias=True,\n",
        "                        kernel_initializer=WEIGHT_INIT,\n",
        "                        bias_initializer=BIAS_INIT,\n",
        "                        name=name)\n",
        "\n",
        "    # run batch norm\n",
        "    x = tf.contrib.layers.batch_norm(inputs=x, activation_fn=activation, is_training=is_training)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class IICGraph(object):\n",
        "    def __init__(self, config='B', batch_norm=True, fan_out_init=64):\n",
        "        \"\"\"\n",
        "        :param config: character {A, B, C} that matches architecture in IIC supplementary materials\n",
        "        :param fan_out_init: initial fan out (paper uses 64, but can be reduced for memory constrained systems)\n",
        "        \"\"\"\n",
        "        # set activation\n",
        "        self.activation = tf.nn.relu\n",
        "\n",
        "        # save architectural details\n",
        "        self.config = config\n",
        "        self.batch_norm = batch_norm\n",
        "        self.fan_out_init = fan_out_init\n",
        "\n",
        "    def __architecture_b(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        with tf.compat.v1.variable_scope('GraphB', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # layer 1\n",
        "            num_out_channels = self.fan_out_init\n",
        "            x = convolution_layer(x=x, kernel_size=5, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1')\n",
        "            x = max_pooling_layer(x=x, pool_size=2, strides=2, name='pool1')\n",
        "\n",
        "            # layer 2\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=5, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2')\n",
        "            x = max_pooling_layer(x=x, pool_size=2, strides=2, name='pool2')\n",
        "\n",
        "            # layer 3\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=5, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3')\n",
        "            x = max_pooling_layer(x=x, pool_size=2, strides=2, name='pool3')\n",
        "\n",
        "            # layer 4\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=5, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4')\n",
        "\n",
        "            # flatten\n",
        "            x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def evaluate(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of graph\n",
        "        \"\"\"\n",
        "        # run corresponding architecture\n",
        "        if self.config == 'B':\n",
        "            return self.__architecture_b(x, is_training)\n",
        "        else:\n",
        "            raise Exception('Unknown graph configuration!')\n",
        "\n",
        "\n",
        "class VGG(object):\n",
        "    def __init__(self, config='A', batch_norm=True, fan_out_init=64):\n",
        "        \"\"\"\n",
        "        :param config: character {A, C, D} that matches architecture in VGG paper\n",
        "        :param fan_out_init: initial fan out (paper uses 64, but can be reduced for memory constrained systems)\n",
        "        \"\"\"\n",
        "        # set activation\n",
        "        self.activation = tf.nn.relu\n",
        "\n",
        "        # save architectural details\n",
        "        self.config = config\n",
        "        self.batch_norm = batch_norm\n",
        "        self.fan_out_init = fan_out_init\n",
        "\n",
        "    def __vgg_a(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        with tf.compat.v1.variable_scope('VGG_A', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "\n",
        "            # layer 1\n",
        "            num_out_channels = self.fan_out_init\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_1')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool1')\n",
        "\n",
        "            # layer 2\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_1')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool2')\n",
        "\n",
        "            # layer 3\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool3')\n",
        "\n",
        "            # layer 4\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool4')\n",
        "\n",
        "            # layer 5\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool5')\n",
        "\n",
        "            # flatten\n",
        "            x = tf.contrib.layers.flatten(x)\n",
        "\n",
        "            # fully connected layers\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc1')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc2')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc3')\n",
        "\n",
        "            return x\n",
        "\n",
        "    def __vgg_c(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        with tf.compat.v1.variable_scope('VGG_C', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "            # layer 1\n",
        "            num_out_channels = self.fan_out_init\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool1')\n",
        "\n",
        "            # layer 2\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool2')\n",
        "\n",
        "            # layer 3\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_2')\n",
        "            x = convolution_layer(x=x, kernel_size=1, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool3')\n",
        "\n",
        "            # layer 4\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_2')\n",
        "            x = convolution_layer(x=x, kernel_size=1, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool4')\n",
        "\n",
        "            # layer 5\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_2')\n",
        "            x = convolution_layer(x=x, kernel_size=1, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool5')\n",
        "\n",
        "            # flatten\n",
        "            x = tf.contrib.layers.flatten(x)\n",
        "\n",
        "            # fully connected layers\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc1')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc2')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc3')\n",
        "\n",
        "            return x\n",
        "\n",
        "    def __vgg_d(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        with tf.compat.v1.variable_scope('VGG_D', reuse=tf.compat.v1.AUTO_REUSE):\n",
        "            # layer 1\n",
        "            num_out_channels = self.fan_out_init\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv1_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool1')\n",
        "\n",
        "            # layer 2\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv2_2')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool2')\n",
        "\n",
        "            # layer 3\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_2')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv3_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool3')\n",
        "\n",
        "            # layer 4\n",
        "            num_out_channels *= 2\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_2')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv4_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool4')\n",
        "\n",
        "            # layer 5\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_1')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_2')\n",
        "            x = convolution_layer(x=x, kernel_size=3, num_out_channels=num_out_channels, activation=self.activation,\n",
        "                                  batch_norm=self.batch_norm, is_training=is_training, name='conv5_3')\n",
        "            x = max_pooling_layer(x=x, pool_size=3, strides=2, name='pool5')\n",
        "\n",
        "            # flatten\n",
        "            x = tf.contrib.layers.flatten(x)\n",
        "\n",
        "            # fully connected layers\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc1')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc2')\n",
        "            x = fully_connected_layer(x=x,\n",
        "                                      num_outputs=4096,\n",
        "                                      activation=self.activation,\n",
        "                                      is_training=is_training,\n",
        "                                      name='fc3')\n",
        "\n",
        "            return x\n",
        "\n",
        "    def evaluate(self, x, is_training):\n",
        "        \"\"\"\n",
        "        :param x: input data\n",
        "        :param is_training: whether we are training or testing (used by batch normalization)\n",
        "        :return: output of VGG\n",
        "        \"\"\"\n",
        "        # run corresponding architecture\n",
        "        if self.config == 'A':\n",
        "            return self.__vgg_a(x, is_training)\n",
        "        elif self.config == 'C':\n",
        "            return self.__vgg_c(x, is_training)\n",
        "        elif self.config == 'D':\n",
        "            return self.__vgg_d(x, is_training)\n",
        "        else:\n",
        "            raise Exception('Unknown VGG configuration!')"
      ],
      "metadata": {
        "id": "SxaZTL__csvj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ],
      "metadata": {
        "id": "hHGLnukBfnPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "\n",
        "def save_performance(perf, epoch, save_path):\n",
        "    \"\"\"\n",
        "    :param perf: performance dictionary\n",
        "    :param epoch: epoch number\n",
        "    :param save_path: path to save plot to. if None, plot will be drawn\n",
        "    :return: none\n",
        "    \"\"\"\n",
        "    # return if save path is None\n",
        "    if save_path is None:\n",
        "        return\n",
        "\n",
        "    # loop over the metrics\n",
        "    for metric in perf.keys():\n",
        "\n",
        "        # loop over the data splits\n",
        "        for split in perf[metric].keys():\n",
        "\n",
        "            # trim data to utilized epochs\n",
        "            perf[metric][split] = perf[metric][split][:epoch]\n",
        "            assert len(perf[metric][split]) == epoch\n",
        "\n",
        "    # create the file name\n",
        "    f_name = os.path.join(save_path, 'perf.pkl')\n",
        "\n",
        "    # pickle it\n",
        "    with open(f_name, 'wb') as f:\n",
        "        pickle.dump(perf, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    # make sure it worked\n",
        "    with open(f_name, 'rb') as f:\n",
        "        perf_load = pickle.load(f)\n",
        "    assert str(perf) == str(perf_load), 'performance saving failed'\n",
        "\n",
        "\n",
        "def unsupervised_labels(y, y_hat, num_classes, num_clusters):\n",
        "    \"\"\"\n",
        "    :param y: true label\n",
        "    :param y_hat: concentration parameter\n",
        "    :param num_classes: number of classes (determined by data)\n",
        "    :param num_clusters: number of clusters (determined by model)\n",
        "    :return: classification error rate\n",
        "    \"\"\"\n",
        "    assert num_classes == num_clusters\n",
        "\n",
        "    # initialize count matrix\n",
        "    cnt_mtx = np.zeros([num_classes, num_classes])\n",
        "\n",
        "    # fill in matrix\n",
        "    for i in range(len(y)):\n",
        "        cnt_mtx[int(y_hat[i]), int(y[i])] += 1\n",
        "\n",
        "    # find optimal permutation\n",
        "    row_ind, col_ind = linear_sum_assignment(-cnt_mtx)\n",
        "\n",
        "    # compute error\n",
        "    error = 1 - cnt_mtx[row_ind, col_ind].sum() / cnt_mtx.sum()\n",
        "\n",
        "    # print results\n",
        "    print('Classification error = {:.4f}'.format(error))\n",
        "\n",
        "    return error"
      ],
      "metadata": {
        "id": "JuLLC0GOfnPp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IIC Model"
      ],
      "metadata": {
        "id": "HU2SQC-yfjoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_slim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57679a86-988c-48f5-f7ac-170ff308c89c",
        "id": "IvlRluiufjo0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf_slim in /usr/local/lib/python3.8/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from tf_slim) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches as patches\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "import tf_slim as slim\n",
        "\n",
        "# plot settings\n",
        "DPI = 600\n",
        "\n",
        "\n",
        "class ClusterIIC(object):\n",
        "    def __init__(self, num_classes, learning_rate, num_repeats, save_dir=None):\n",
        "        \"\"\"\n",
        "        :param num_classes: number of classes\n",
        "        :param learning_rate: gradient step size\n",
        "        :param num_repeats: number of data repeats for x and g(x), used to up-sample\n",
        "        \"\"\"\n",
        "        # save configuration\n",
        "        self.k_A = 5 * num_classes\n",
        "        self.num_A_sub_heads = 1\n",
        "        self.k_B = num_classes\n",
        "        self.num_B_sub_heads = 5\n",
        "        self.num_repeats = num_repeats\n",
        "\n",
        "        # initialize losses\n",
        "        self.loss_A = None\n",
        "        self.loss_B = None\n",
        "        self.losses = []\n",
        "\n",
        "        # initialize outputs\n",
        "        self.y_hats = None\n",
        "\n",
        "        # initialize optimizer\n",
        "        self.is_training = tf.compat.v1.placeholder(tf.bool)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        self.opt = tf.compat.v1.train.AdamOptimizer(self.learning_rate)\n",
        "        self.train_ops = []\n",
        "\n",
        "        # initialize performance dictionary\n",
        "        self.perf = None\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        # configure performance plotting\n",
        "        self.fig_learn, self.ax_learn = plt.subplots(1, 2)\n",
        "\n",
        "    def __iic_loss(self, pi_x, pi_gx):\n",
        "\n",
        "        # up-sample non-perturbed to match the number of repeat samples\n",
        "        pi_x = tf.tile(pi_x, [self.num_repeats] + [1] * len(pi_x.shape.as_list()[1:]))\n",
        "\n",
        "        # get K\n",
        "        k = pi_x.shape.as_list()[1]\n",
        "\n",
        "        # compute P\n",
        "        p = tf.transpose(pi_x) @ pi_gx\n",
        "\n",
        "        # enforce symmetry\n",
        "        p = (p + tf.transpose(p)) / 2\n",
        "\n",
        "        # enforce minimum value\n",
        "        p = tf.clip_by_value(p, clip_value_min=1e-6, clip_value_max=tf.float32.max)\n",
        "\n",
        "        # normalize\n",
        "        p /= tf.reduce_sum(p)\n",
        "\n",
        "        # get marginals\n",
        "        pi = tf.broadcast_to(tf.reshape(tf.reduce_sum(p, axis=0), (k, 1)), (k, k))\n",
        "        pj = tf.broadcast_to(tf.reshape(tf.reduce_sum(p, axis=1), (1, k)), (k, k))\n",
        "\n",
        "        # complete the loss\n",
        "        loss = -tf.reduce_sum(p * (tf.math.log(p) - tf.math.log(pi) - tf.math.log(pj)))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def __head_out(z, k, name):\n",
        "\n",
        "        # construct a new head that operates on the model's output for x\n",
        "        with tf.compat.v1.variable_scope(name, reuse=tf.compat.v1.AUTO_REUSE):\n",
        "            phi = tf.compat.v1.layers.dense(\n",
        "                inputs=z,\n",
        "                units=k,\n",
        "                activation=tf.nn.softmax,\n",
        "                use_bias=True,\n",
        "                kernel_initializer=KERNEL_INIT,\n",
        "                bias_initializer=BIAS_INIT)\n",
        "\n",
        "        return phi\n",
        "\n",
        "    def __head_loss(self, z_x, z_gx, k, num_sub_heads, head):\n",
        "\n",
        "        # loop over the number of sub-heads\n",
        "        loss = tf.constant(0, dtype=tf.float32)\n",
        "        for i in range(num_sub_heads):\n",
        "\n",
        "            # run the model\n",
        "            pi_x = self.__head_out(z_x, k, name=head + str(i + 1))\n",
        "            num_vars = len(tf.compat.v1.global_variables())\n",
        "            pi_gx = self.__head_out(z_gx, k, name=head + str(i + 1))\n",
        "            assert num_vars == len(tf.compat.v1.global_variables())\n",
        "\n",
        "            # accumulate the clustering loss\n",
        "            loss += self.__iic_loss(pi_x, pi_gx)\n",
        "\n",
        "        # take the average\n",
        "        if num_sub_heads > 0:\n",
        "            loss /= num_sub_heads\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def __build(self, x, gx, graph):\n",
        "\n",
        "        # run the graph\n",
        "        z_x = graph.evaluate(x, is_training=self.is_training)\n",
        "        num_vars = len(tf.compat.v1.global_variables())\n",
        "        z_gx = graph.evaluate(gx, is_training=self.is_training)\n",
        "        assert num_vars == len(tf.compat.v1.global_variables())\n",
        "\n",
        "        # construct losses\n",
        "        self.loss_A = self.__head_loss(z_x, z_gx, self.k_A, self.num_A_sub_heads, 'A')\n",
        "        self.loss_B = self.__head_loss(z_x, z_gx, self.k_B, self.num_B_sub_heads, 'B')\n",
        "        self.losses = [self.loss_A, self.loss_B]\n",
        "\n",
        "        # set alternating training operations\n",
        "        self.train_ops.append(slim.optimize_loss(loss=self.loss_A,\n",
        "                                                              global_step=self.global_step,\n",
        "                                                              learning_rate=self.learning_rate,\n",
        "                                                              optimizer=self.opt,\n",
        "                                                              summaries=['loss', 'gradients']))\n",
        "        self.train_ops.append(slim.optimize_loss(loss=self.loss_B,\n",
        "                                                              global_step=self.global_step,\n",
        "                                                              learning_rate=self.learning_rate,\n",
        "                                                              optimizer=self.opt,\n",
        "                                                              summaries=['loss', 'gradients']))\n",
        "\n",
        "        # initialize outputs outputs\n",
        "        self.y_hats = [tf.argmax(self.__head_out(z_x, self.k_B, 'B' + str(i + 1)), axis=1)\n",
        "                       for i in range(self.num_B_sub_heads)]\n",
        "\n",
        "    def __performance_dictionary_init(self, num_epochs):\n",
        "        \"\"\"\n",
        "        :param num_epochs: maximum number of epochs (used to size buffers)\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # initialize performance dictionary\n",
        "        self.perf = dict()\n",
        "\n",
        "        # loss terms\n",
        "        self.perf.update({'loss_A': np.zeros(num_epochs)})\n",
        "        self.perf.update({'loss_B': np.zeros(num_epochs)})\n",
        "\n",
        "        # classification error\n",
        "        self.perf.update({'class_err_min': np.zeros(num_epochs)})\n",
        "        self.perf.update({'class_err_avg': np.zeros(num_epochs)})\n",
        "        self.perf.update({'class_err_max': np.zeros(num_epochs)})\n",
        "\n",
        "    def __classification_accuracy(self, sess, iter_init, idx, y_ph=None):\n",
        "        \"\"\"\n",
        "        :param sess: TensorFlow session\n",
        "        :param iter_init: TensorFlow data iterator initializer associated\n",
        "        :param idx: insertion index (i.e. epoch - 1)\n",
        "        :param y_ph: TensorFlow placeholder for unseen labels\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        if self.perf is None or y_ph is None:\n",
        "            return\n",
        "\n",
        "        # initialize results\n",
        "        y = np.zeros([0, 1])\n",
        "        y_hats = [np.zeros([0, 1])] * self.num_B_sub_heads\n",
        "\n",
        "        # initialize unsupervised data iterator\n",
        "        sess.run(iter_init)\n",
        "\n",
        "        # loop over the batches within the unsupervised data iterator\n",
        "        print('Evaluating classification accuracy... ')\n",
        "        while True:\n",
        "            try:\n",
        "                # grab the results\n",
        "                results = sess.run([self.y_hats, y_ph], feed_dict={self.is_training: False})\n",
        "\n",
        "                # load metrics\n",
        "                for i in range(self.num_B_sub_heads):\n",
        "                    y_hats[i] = np.concatenate((y_hats[i], np.expand_dims(results[0][i], axis=1)))\n",
        "                if y_ph is not None:\n",
        "                    y = np.concatenate((y, np.expand_dims(results[1], axis=1)))\n",
        "\n",
        "                # _, ax = plt.subplots(2, 10)\n",
        "                # i_rand = np.random.choice(results[3].shape[0], 10)\n",
        "                # for i in range(10):\n",
        "                #     ax[0, i].imshow(results[3][i_rand[i]][:, :, 0], origin='upper', vmin=0, vmax=1)\n",
        "                #     ax[0, i].set_xticks([])\n",
        "                #     ax[0, i].set_yticks([])\n",
        "                #     ax[1, i].imshow(results[4][i_rand[i]][:, :, 0], origin='upper', vmin=0, vmax=1)\n",
        "                #     ax[1, i].set_xticks([])\n",
        "                #     ax[1, i].set_yticks([])\n",
        "                # plt.show()\n",
        "\n",
        "            # iterator will throw this error when its out of data\n",
        "            except tf.errors.OutOfRangeError:\n",
        "                break\n",
        "\n",
        "        # compute classification accuracy\n",
        "        if y_ph is not None:\n",
        "            class_errors = [unsupervised_labels(y, y_hats[i], self.k_B, self.k_B)\n",
        "                            for i in range(self.num_B_sub_heads)]\n",
        "            self.perf['class_err_min'][idx] = np.min(class_errors)\n",
        "            self.perf['class_err_avg'][idx] = np.mean(class_errors)\n",
        "            self.perf['class_err_max'][idx] = np.max(class_errors)\n",
        "\n",
        "        # metrics are done\n",
        "        print('Done')\n",
        "\n",
        "    def plot_learning_curve(self, epoch):\n",
        "        \"\"\"\n",
        "        :param epoch: epoch number\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # generate epoch numbers\n",
        "        t = np.arange(1, epoch + 1)\n",
        "\n",
        "        # colors\n",
        "        c = {'Head A': '#1f77b4', 'Head B': '#ff7f0e'}\n",
        "\n",
        "        # plot the loss\n",
        "        self.ax_learn[0].clear()\n",
        "        self.ax_learn[0].set_title('Loss')\n",
        "        self.ax_learn[0].plot(t, self.perf['loss_A'][:epoch], label='Head A', color=c['Head A'])\n",
        "        self.ax_learn[0].plot(t, self.perf['loss_B'][:epoch], label='Head B', color=c['Head B'])\n",
        "        self.ax_learn[0].xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        self.ax_learn[0].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "\n",
        "        # plot the classification error\n",
        "        self.ax_learn[1].clear()\n",
        "        self.ax_learn[1].set_title('Class. Error (Min, Avg, Max)')\n",
        "        self.ax_learn[1].plot(t, self.perf['class_err_avg'][:epoch], color=c['Head B'])\n",
        "        self.ax_learn[1].fill_between(t,\n",
        "                                      self.perf['class_err_min'][:epoch],\n",
        "                                      self.perf['class_err_max'][:epoch],\n",
        "                                      facecolor=c['Head B'], alpha=0.5)\n",
        "        self.ax_learn[1].plot(t, self.perf['class_err_avg'][:epoch], color=c['Head B'])\n",
        "        self.ax_learn[1].fill_between(t,\n",
        "                                      self.perf['class_err_min'][:epoch],\n",
        "                                      self.perf['class_err_max'][:epoch],\n",
        "                                      facecolor=c['Head B'], alpha=0.5)\n",
        "        self.ax_learn[1].xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        self.ax_learn[1].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "\n",
        "        # make the legend\n",
        "        self.ax_learn[1].legend(handles=[patches.Patch(color=val, label=key) for key, val in c.items()],\n",
        "                                ncol=len(c),\n",
        "                                bbox_to_anchor=(0.35, -0.06))\n",
        "\n",
        "        # eliminate those pesky margins\n",
        "        self.fig_learn.subplots_adjust(left=0.1, bottom=0.15, right=0.95, top=0.95, wspace=0.25, hspace=0.3)\n",
        "\n",
        "    def train(self, graph, train_set, test_set, num_epochs, early_stop_buffer=15):\n",
        "        \"\"\"\n",
        "        :param graph: the computational graph\n",
        "        :param train_set: TensorFlow Dataset object that corresponds to training data\n",
        "        :param test_set: TensorFlow Dataset object that corresponds to validation data\n",
        "        :param num_epochs: number of epochs\n",
        "        :param early_stop_buffer: early stop look-ahead distance (in epochs)\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        # construct iterator\n",
        "        iterator = tf.compat.v1.data.make_initializable_iterator(train_set)\n",
        "        x, gx, y = iterator.get_next().values()\n",
        "\n",
        "        # construct initialization operations\n",
        "        train_iter_init = iterator.make_initializer(train_set)\n",
        "        test_iter_init = iterator.make_initializer(test_set)\n",
        "\n",
        "        # build the model using the supplied computational graph\n",
        "        self.__build(x, gx, graph)\n",
        "\n",
        "        # initialize performance dictionary\n",
        "        self.__performance_dictionary_init(num_epochs)\n",
        "\n",
        "        # start a monitored session\n",
        "        cfg = tf.compat.v1.ConfigProto()\n",
        "        cfg.gpu_options.allow_growth = True\n",
        "        with tf.compat.v1.Session(config=cfg) as sess:\n",
        "\n",
        "            # initialize model variables\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "            # loop over the number of epochs\n",
        "            for i in range(num_epochs):\n",
        "\n",
        "                # start timer\n",
        "                start = time.time()\n",
        "\n",
        "                # get epoch number\n",
        "                epoch = i + 1\n",
        "\n",
        "                # get training operation\n",
        "                i_train = i % len(self.train_ops)\n",
        "\n",
        "                # initialize epoch iterator\n",
        "                sess.run(train_iter_init)\n",
        "\n",
        "                # loop over the batches\n",
        "                loss_A = []\n",
        "                loss_B = []\n",
        "                while True:\n",
        "                    try:\n",
        "\n",
        "                        # run training and losses\n",
        "                        losses = sess.run([self.train_ops[i_train]] + [self.losses],\n",
        "                                          feed_dict={self.is_training: True})[-1]\n",
        "\n",
        "                        # load metrics\n",
        "                        loss_A.append(losses[0])\n",
        "                        loss_B.append(losses[1])\n",
        "\n",
        "                        if np.isnan(losses).any():\n",
        "                            print('\\n NaN whelp!')\n",
        "                            return\n",
        "\n",
        "                        # print update\n",
        "                        print('\\rEpoch {:d}, Loss = {:.4f}'.format(epoch, losses[i_train]), end='')\n",
        "\n",
        "                    # iterator will throw this error when its out of data\n",
        "                    except tf.errors.OutOfRangeError:\n",
        "                        break\n",
        "\n",
        "                # new line\n",
        "                print('')\n",
        "\n",
        "                # save averaged training performance\n",
        "                self.perf['loss_A'][i] = np.mean(loss_A)\n",
        "                self.perf['loss_B'][i] = np.mean(loss_B)\n",
        "\n",
        "                # get classification performance\n",
        "                self.__classification_accuracy(sess, test_iter_init, i, y)\n",
        "\n",
        "                # plot learning curve\n",
        "                self.plot_learning_curve(epoch)\n",
        "\n",
        "                # pause for plot drawing if we aren't saving\n",
        "                if self.save_dir is None:\n",
        "                    plt.pause(0.05)\n",
        "\n",
        "                # print time for epoch\n",
        "                stop = time.time()\n",
        "                print('Time for Epoch = {:f}'.format(stop - start))\n",
        "\n",
        "                # early stop check\n",
        "                # i_best_elbo = np.argmin(self.perf['loss']['test'][:epoch])\n",
        "                # i_best_class = np.argmin(self.perf['class_err']['test'][:epoch])\n",
        "                # epochs_since_improvement = min(i - i_best_elbo, i - i_best_class)\n",
        "                # print('Early stop checks: {:d} / {:d}\\n'.format(epochs_since_improvement, early_stop_buffer))\n",
        "                # if epochs_since_improvement >= early_stop_buffer:\n",
        "                #     break\n",
        "\n",
        "        # save the performance\n",
        "        save_performance(self.perf, epoch, self.save_dir)"
      ],
      "metadata": {
        "id": "b63yaF9ofjo1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run"
      ],
      "metadata": {
        "id": "0-mZzaYjJoQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# pick a data set\n",
        "DATA_SET = 'german_sign'\n",
        "\n",
        "# define splits\n",
        "DS_CONFIG = {\n",
        "    # mnist data set parameters\n",
        "    'german_sign': {\n",
        "        'batch_size': 700,\n",
        "        'num_repeats': 5,\n",
        "        'mdl_input_dims': [32, 32, 3]}\n",
        "}\n",
        "\n",
        "# load the data set\n",
        "TRAIN_SET, TEST_SET, SET_INFO = load(X_train, y_train, X_test, y_test, **DS_CONFIG[DATA_SET])\n",
        "\n",
        "# configure the common model elements\n",
        "MDL_CONFIG = {\n",
        "    # mist hyper-parameters\n",
        "    'german_sign': {\n",
        "        'num_classes': 43,\n",
        "        'learning_rate': 1e-2,\n",
        "        'num_repeats': DS_CONFIG[DATA_SET]['num_repeats'],\n",
        "        'save_dir': None},\n",
        "}\n",
        "\n",
        "# declare the model\n",
        "mdl = ClusterIIC(**MDL_CONFIG[DATA_SET])\n",
        "\n",
        "# train the model\n",
        "mdl.train(IICGraph(config='B', batch_norm=True, fan_out_init=64), TRAIN_SET, TEST_SET, num_epochs=10)\n",
        "\n",
        "print('All done!')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "id": "LX94_MtIJrrO",
        "outputId": "4318c933-6b77-4bb3-d9f5-d89e75192dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py:360: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py:361: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py:363: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "<ipython-input-12-af2e7126ff5c>:21: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  x = tf.compat.v1.layers.conv2d(inputs=x,\n",
            "<ipython-input-12-af2e7126ff5c>:50: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  x = tf.compat.v1.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=strides, padding='same', name=name)\n",
            "<ipython-input-15-5a66e2755dbb>:84: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  phi = tf.compat.v1.layers.dense(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss = -0.6497\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.9421\n",
            "Classification error = 0.9421\n",
            "Classification error = 0.9421\n",
            "Classification error = 0.9421\n",
            "Classification error = 0.9421\n",
            "Done\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for Epoch = 5514.380227\n",
            "Epoch 2, Loss = -0.5967\n",
            "Evaluating classification accuracy... \n",
            "Classification error = 0.9421\n",
            "Classification error = 0.9421\n",
            "Classification error = 0.9421\n",
            "Classification error = 0.9421\n",
            "Classification error = 0.9421\n",
            "Done\n",
            "Time for Epoch = 5627.117682\n",
            "Epoch 3, Loss = -0.6550"
          ]
        }
      ]
    }
  ]
}